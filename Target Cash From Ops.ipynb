{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from supervised.automl import AutoML\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DF Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'canada_updated.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_canada \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcanada_updated.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m df_canada\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/anaconda3/envs/HECFinance/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/envs/HECFinance/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/HECFinance/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/envs/HECFinance/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/HECFinance/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'canada_updated.csv'"
     ]
    }
   ],
   "source": [
    "df_canada = pd.read_csv('canada_updated.csv')\n",
    "df_canada.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copie pour modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_canada.copy()\n",
    "\n",
    "df_model['date'] = pd.to_datetime(df_model['date'], errors='coerce')\n",
    "df_model.sort_values(by=['cid', 'date'], inplace=True)\n",
    "\n",
    "# Retirer les lignes où Quality_Flag est False\n",
    "df_model = df_model[df_model['QUALITY_FLAG'] == True]\n",
    "\n",
    "# (FACULTATIF) Exclure les banques\n",
    "# df_model = df_model[df_model['industry'] != 'Banks']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(\n",
    "    df,\n",
    "    include_agro=False,\n",
    "    include_rgro=False,\n",
    "    include_tcgro=False,\n",
    "    include_ratios_assets=False,\n",
    "    include_ratios_rev=False,\n",
    "    include_ratios_totcap=False,\n",
    "    mandatory_cols=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Sélectionne dynamiquement les colonnes d'un DataFrame en fonction\n",
    "    des familles de variables explicatives demandées,\n",
    "    en plaçant d'abord les colonnes obligatoires (mandatory_cols).\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1) Gérer la liste mandatory_cols (par défaut : vide ou ['cid','date'] selon besoin)\n",
    "    if mandatory_cols is None:\n",
    "        mandatory_cols = []\n",
    "    \n",
    "    # 2) Définir les \"familles\" de motifs\n",
    "    family_patterns = {\n",
    "        'agro': ['_agro_1q', '_agro_4q'],\n",
    "        'rgro': ['_rgro_1q', '_rgro_4q'],\n",
    "        'tcgro': ['_tcgro_1q', '_tcgro_4q'],\n",
    "        'ratios_assets': ['_on_assets_ratio'],\n",
    "        'ratios_rev': ['_on_rev_ratio'],\n",
    "        'ratios_totcap': ['_on_tot_cap_ratio']\n",
    "    }\n",
    "    \n",
    "    # 3) Construire la liste des motifs à inclure\n",
    "    patterns_to_keep = []\n",
    "    if include_agro:\n",
    "        patterns_to_keep += family_patterns['agro']\n",
    "    if include_rgro:\n",
    "        patterns_to_keep += family_patterns['rgro']\n",
    "    if include_tcgro:\n",
    "        patterns_to_keep += family_patterns['tcgro']\n",
    "    \n",
    "    if include_ratios_assets:\n",
    "        patterns_to_keep += family_patterns['ratios_assets']\n",
    "    if include_ratios_rev:\n",
    "        patterns_to_keep += family_patterns['ratios_rev']\n",
    "    if include_ratios_totcap:\n",
    "        patterns_to_keep += family_patterns['ratios_totcap']\n",
    "    \n",
    "    # 4) Retrouver toutes les colonnes du df qui matchent nos motifs\n",
    "    matched_cols = set()\n",
    "    for pat in patterns_to_keep:\n",
    "        for col in df.columns:\n",
    "            if pat in col:\n",
    "                matched_cols.add(col)\n",
    "    # => matched_cols est un set() de colonnes\n",
    "    \n",
    "    # 5) Conserver l'ordre original des colonnes matched, \n",
    "    #    en filtrant df.columns dans l'ordre d'origine\n",
    "    matched_cols_in_order = [c for c in df.columns if c in matched_cols]\n",
    "    \n",
    "    # 6) Construire l'ordre final :\n",
    "    #    - d'abord mandatory_cols (dans l'ordre donné),\n",
    "    #    - puis les matched_cols (dans l'ordre d'origine)\n",
    "    #    - attention aux colonnes obligatoires qui n'existent pas, \n",
    "    #      ou aux duplications\n",
    "    #    - on fait donc une intersection + un set() pour éviter \n",
    "    #      les collisions.\n",
    "    \n",
    "    # Intersection pour ne pas inclure des mandatory inexistantes\n",
    "    mandatory_cols_in_df = [c for c in mandatory_cols if c in df.columns]\n",
    "    \n",
    "    # Puis on concatène en évitant toute duplication\n",
    "    columns_to_keep_ordered = mandatory_cols_in_df + [\n",
    "        c for c in matched_cols_in_order if c not in mandatory_cols_in_df\n",
    "    ]\n",
    "    \n",
    "    # 7) Extraire le sous-DataFrame\n",
    "    df_filtered = df[columns_to_keep_ordered].copy()\n",
    "    \n",
    "    # 8) (Optionnel) trier par cid/date si elles sont présentes\n",
    "    if 'cid' in df_filtered.columns and 'date' in df_filtered.columns:\n",
    "        df_filtered.sort_values(by=['cid', 'date'], inplace=True)\n",
    "    \n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_final = select_features(\n",
    "    df_model,\n",
    "    include_agro=True,\n",
    "    include_rgro=True,\n",
    "    include_tcgro=True,\n",
    "    include_ratios_assets=True,\n",
    "    include_ratios_rev=True,\n",
    "    include_ratios_totcap=True,\n",
    "    mandatory_cols=['cid', 'date', 'binary_target_cash_operations']  # je garde la target\n",
    ")\n",
    "\n",
    "# Compter le nombre total de NaN dans tout le DataFrame\n",
    "total_nan = df_model_final.isna().sum().sum()\n",
    "print(f\"Nombre total de valeurs NaN dans df_model_final : {total_nan}\")\n",
    "\n",
    "# Retirer les lignes qui contiennent AU MOINS un NaN\n",
    "df_model_final.dropna(inplace=True)\n",
    "\n",
    "# Vérifier à nouveau qu’il n’y a plus de NaN\n",
    "total_nan_apres = df_model_final.isna().sum().sum()\n",
    "print(f\"Nombre total de valeurs NaN après suppression : {total_nan_apres}\")\n",
    "\n",
    "print(df_model_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DF Test pour tester la fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date la plus récente du DataFrame\n",
    "max_date = df_model_final['date'].max()\n",
    "\n",
    "# Date de coupure (5 ans avant)\n",
    "cutoff_date = max_date - pd.DateOffset(years=5)\n",
    "\n",
    "# Filtrer pour ne garder que les 5 dernières années\n",
    "df_test = df_model_final[df_model_final['date'] >= cutoff_date].copy()\n",
    "\n",
    "print(df_test['date'].min(), df_test['date'].max())\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Window + AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_rolling_windows(data, date_col, target_col, train_years, val_years, test_years, buffer_months=0):\n",
    "    \"\"\"\n",
    "    Pipeline direct pour la rolling window avec AutoML et cross-validation personnalisée.\n",
    "    Ajoute les périodes dans le DataFrame final pour validation.\n",
    "    \"\"\"\n",
    "    # Conversion de la colonne date\n",
    "    data[date_col] = pd.to_datetime(data[date_col])\n",
    "    start_date = data[date_col].min()\n",
    "    end_date = data[date_col].max()\n",
    "\n",
    "    predictions_all = []  # Liste pour stocker toutes les prédictions\n",
    "\n",
    "    while start_date + relativedelta(years=train_years + val_years + test_years) <= end_date:\n",
    "        # Définir les périodes\n",
    "        train_end = start_date + relativedelta(years=train_years) - pd.Timedelta(days=1)\n",
    "        tampon_1_end = train_end + relativedelta(months=buffer_months)\n",
    "        val_start = tampon_1_end + pd.Timedelta(days=1)\n",
    "        val_end = val_start + relativedelta(years=val_years) - pd.Timedelta(days=1)\n",
    "        tampon_2_end = val_end + relativedelta(months=buffer_months)\n",
    "        test_start = tampon_2_end + pd.Timedelta(days=1)\n",
    "        test_end = test_start + relativedelta(years=test_years) - pd.Timedelta(days=1)\n",
    "\n",
    "        # Filtrer les données\n",
    "        train_data = data.loc[(data[date_col] >= start_date) & (data[date_col] <= train_end)]\n",
    "        val_data = data.loc[(data[date_col] >= val_start) & (data[date_col] <= val_end)]\n",
    "        test_data = data.loc[(data[date_col] >= test_start) & (data[date_col] <= test_end)]\n",
    "\n",
    "        if len(train_data) == 0 or len(val_data) == 0 or len(test_data) == 0:\n",
    "            print(f\"Fenêtre {start_date.year}-{test_end.year} : données insuffisantes, sautée.\")\n",
    "            start_date += relativedelta(years=1)\n",
    "            continue\n",
    "\n",
    "        # Configurer et entraîner AutoML\n",
    "        print(f\"Fenêtre {start_date.year}-{test_end.year} : entraînement de AutoML...\")\n",
    "        automl = AutoML(results_path=f\"AutoML_{start_date.year}-{test_end.year}\", mode=\"Perform\", algorithms=[\"Xgboost\"], eval_metric=\"auc\")\n",
    "        custom_cv = [(train_data.index, val_data.index)]\n",
    "        automl.fit(\n",
    "            train_data.drop(columns=[target_col, date_col, 'cid']),\n",
    "            train_data[target_col], cv=custom_cv\n",
    "        )\n",
    "\n",
    "        # Prédire sur le test set\n",
    "        test_preds = test_data[[date_col, target_col]].copy()\n",
    "        #test_preds[\"predicted\"] = automl.predict_proba(test_data.drop(columns=[target_col, date_col, 'cid']))\n",
    "        proba = automl.predict_proba(test_data.drop(columns=[target_col, date_col, 'cid']))\n",
    "        test_preds[\"proba_class_0\"] = proba[:, 0]  # Probabilité pour la classe 0 (diminution des bénéfices)\n",
    "        test_preds[\"proba_class_1\"] = proba[:, 1]  # Probabilité pour la classe 1 (augmentation des bénéfices)\n",
    "        test_preds[\"margin_proba\"] = test_preds[\"proba_class_1\"] - test_preds[\"proba_class_0\"]\n",
    "        test_preds[\"window\"] = f\"{start_date.year}-{test_end.year}\"\n",
    "        test_preds[\"cid\"] = test_data[\"cid\"].values\n",
    "\n",
    "        # Ajouter les périodes pour validation\n",
    "        #test_preds[\"train_start\"] = start_date\n",
    "        #test_preds[\"train_end\"] = train_end\n",
    "        #test_preds[\"tampon_1\"] = tampon_1_end\n",
    "        #test_preds[\"val_start\"] = val_start\n",
    "        #test_preds[\"val_end\"] = val_end\n",
    "        #test_preds[\"tampon_2\"] = tampon_2_end\n",
    "        #test_preds[\"test_start\"] = test_start\n",
    "        #test_preds[\"test_end\"] = test_end\n",
    "\n",
    "        # Sauvegarder les prédictions\n",
    "        predictions_all.append(test_preds)\n",
    "\n",
    "        # Avancer la fenêtre\n",
    "        start_date += relativedelta(years=1)\n",
    "\n",
    "    predictions_df = pd.concat(predictions_all, ignore_index=True)\n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appel de fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pipeline_rolling_windows(\n",
    "    data=df_test, \n",
    "    date_col=\"date\", \n",
    "    target_col=\"binary_target_cash_operations\", \n",
    "    train_years=2, \n",
    "    val_years=1, \n",
    "    test_years=1, \n",
    "    buffer_months=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.to_csv(\"df_can_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DF Merged pour avoir les rendements associés aux prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assure que les colonnes 'date' sont au bon format datetime dans les deux DataFrames\n",
    "predictions_df['date'] = pd.to_datetime(predictions_df['date'])\n",
    "df_canada['date'] = pd.to_datetime(df_canada['date'])\n",
    "\n",
    "# Faire la jointure sur 'cid' et 'date'\n",
    "merged_df = predictions_df.merge(df_canada[['cid', 'date', 'return_1q']], on=['cid', 'date'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création de Portefeuille + Weighted Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weighted_portfolios(predictions_df, df_canada, proba_col, return_col, lower_threshold=0.4, upper_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Crée un portefeuille pondéré basé sur les prédictions et calcule les rendements pondérés par année,\n",
    "    en attribuant des poids positifs aux positions longues et négatifs aux positions courtes, avec une somme neutre.\n",
    "    \n",
    "    Args:\n",
    "    - predictions_df (pd.DataFrame): DataFrame contenant les prédictions et les identifiants 'cid'.\n",
    "    - df_canada (pd.DataFrame): DataFrame contenant les rendements futurs et les identifiants 'cid'.\n",
    "    - return_col (str): Nom de la colonne des rendements futurs dans df_canada.\n",
    "    - lower_threshold (float): Seuil inférieur pour les positions courtes.\n",
    "    - upper_threshold (float): Seuil supérieur pour les positions longues.\n",
    "    \n",
    "    Returns:\n",
    "    - result_df (pd.DataFrame): DataFrame contenant les rendements pondérés des portefeuilles par année.\n",
    "    \"\"\"\n",
    "    # Joindre les deux DataFrames sur 'cid' et 'date'\n",
    "    merged_df = predictions_df.merge(df_canada[['cid', 'date', return_col]], on=['cid', 'date'], how='left')\n",
    "    \n",
    "    # Extraire l'année à partir de la colonne 'date'\n",
    "    merged_df['year'] = merged_df['date'].dt.year\n",
    "    \n",
    "    # Retirer les lignes avec des valeurs manquantes\n",
    "    merged_df.dropna(inplace=True)\n",
    "    \n",
    "    # Initialiser une liste pour stocker les résultats\n",
    "    results = []\n",
    "\n",
    "    # Grouper par année\n",
    "    for year, group in merged_df.groupby('year'):\n",
    "        # Sélectionner les positions longues et courtes selon les seuils\n",
    "        selected_long = group[group[proba_col] > upper_threshold]  # Positions longues\n",
    "        selected_short = group[group[proba_col] < lower_threshold]  # Positions courtes\n",
    "        \n",
    "        n_long = len(selected_long)\n",
    "        n_short = len(selected_short)\n",
    "        \n",
    "        if n_long > 0 or n_short > 0:\n",
    "            # Attribuer des poids aux positions longues et courtes\n",
    "            if n_long > 0:\n",
    "                selected_long.loc[:, 'weight'] = 1 / n_long  # Poids positifs pour les positions longues\n",
    "            if n_short > 0:\n",
    "                selected_short.loc[:, 'weight'] = -1 / n_short  # Poids négatifs pour les positions courtes\n",
    "            \n",
    "            # Combiner les deux DataFrames\n",
    "            selected = pd.concat([selected_long, selected_short], ignore_index=True)\n",
    "            \n",
    "            # Calculer le rendement pondéré du portefeuille (somme des rendements pondérés)\n",
    "            weighted_return = (selected['weight'] * selected[return_col] * 100).sum()\n",
    "            \n",
    "            # Ajouter le résultat à la liste\n",
    "            results.append({'year': year, 'weighted_return': weighted_return})\n",
    "        else:\n",
    "            # Si aucune action ne respecte les seuils, le rendement est NaN\n",
    "            results.append({'year': year, 'weighted_return': float('nan')})\n",
    "\n",
    "    # Convertir les résultats en DataFrame\n",
    "    result_df = pd.DataFrame(results)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appel de fonction 0.5 // 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_weighted_portfolios' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_returns_margin_proba \u001b[38;5;241m=\u001b[39m create_weighted_portfolios(predictions_df, df_canada, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmargin_porba\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturn_1q\u001b[39m\u001b[38;5;124m'\u001b[39m, lower_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, upper_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m      3\u001b[0m test_returns_margin_proba\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturns_net_income_0.5_0.5.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m test_returns_margin_proba\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_weighted_portfolios' is not defined"
     ]
    }
   ],
   "source": [
    "ret_net_income = create_weighted_portfolios(predictions_df, df_canada, 'margin_porba', 'return_1q', lower_threshold=0.5, upper_threshold=0.5)\n",
    "\n",
    "#ret_net_income.to_csv(\"returns_net_income_0.5_0.5.csv\", index=False)\n",
    "\n",
    "ret_net_income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appel de fonction 0.4 // 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_net_income = create_weighted_portfolios(predictions_df, df_canada, 'margin_porba', 'return_1q', lower_threshold=0.4, upper_threshold=0.6)\n",
    "\n",
    "#returns_net_income.to_csv(\"returns_net_income_0.4_0.6.csv\", index=False)\n",
    "\n",
    "returns_net_income"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HECFinance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
