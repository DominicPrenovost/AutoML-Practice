{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from supervised.automl import AutoML\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DF Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>QUALITY_FLAG</th>\n",
       "      <th>cid</th>\n",
       "      <th>industry_raw</th>\n",
       "      <th>E_TTM_period_date</th>\n",
       "      <th>E_TTM_ammor_intangibles</th>\n",
       "      <th>E_TTM_asset_writedown</th>\n",
       "      <th>E_TTM_assets_gro_five</th>\n",
       "      <th>E_TTM_capex</th>\n",
       "      <th>E_TTM_cash_acquisitions</th>\n",
       "      <th>...</th>\n",
       "      <th>E_G_ebitda_cov</th>\n",
       "      <th>E_G_ret_on_asset</th>\n",
       "      <th>E_G_ret_on_inv_cap</th>\n",
       "      <th>E_G_net_to_cash</th>\n",
       "      <th>E_G_perm_assets_ratio</th>\n",
       "      <th>return_1q</th>\n",
       "      <th>target_net_income</th>\n",
       "      <th>target_cash_operations</th>\n",
       "      <th>binary_target_net_income</th>\n",
       "      <th>binary_target_cash_operations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2002-01-03</td>\n",
       "      <td>True</td>\n",
       "      <td>SP-065996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001-10-31</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.738000</td>\n",
       "      <td>-3.336</td>\n",
       "      <td>...</td>\n",
       "      <td>-165.453488</td>\n",
       "      <td>0.130018</td>\n",
       "      <td>0.101871</td>\n",
       "      <td>-0.068216</td>\n",
       "      <td>0.414230</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2002-01-08</td>\n",
       "      <td>True</td>\n",
       "      <td>SP-002396</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001-09-30</td>\n",
       "      <td>3.078</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.889000</td>\n",
       "      <td>-68.220</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.685925</td>\n",
       "      <td>0.071119</td>\n",
       "      <td>0.067430</td>\n",
       "      <td>-0.004881</td>\n",
       "      <td>0.595752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002-01-08</td>\n",
       "      <td>True</td>\n",
       "      <td>SP-006704</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001-09-30</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-17.971623</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.262460</td>\n",
       "      <td>-0.069781</td>\n",
       "      <td>-0.039238</td>\n",
       "      <td>-0.045993</td>\n",
       "      <td>0.775432</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2002-01-08</td>\n",
       "      <td>True</td>\n",
       "      <td>SP-008644</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001-09-30</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-34.700000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.852273</td>\n",
       "      <td>-0.169833</td>\n",
       "      <td>-0.155712</td>\n",
       "      <td>-0.316372</td>\n",
       "      <td>0.773996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2002-01-08</td>\n",
       "      <td>True</td>\n",
       "      <td>SP-013994</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001-09-30</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1403.000000</td>\n",
       "      <td>-133.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.569697</td>\n",
       "      <td>0.109798</td>\n",
       "      <td>0.078497</td>\n",
       "      <td>-0.157934</td>\n",
       "      <td>0.921832</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 670 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  QUALITY_FLAG        cid industry_raw E_TTM_period_date  \\\n",
       "0 2002-01-03          True  SP-065996          NaN        2001-10-31   \n",
       "1 2002-01-08          True  SP-002396          NaN        2001-09-30   \n",
       "2 2002-01-08          True  SP-006704          NaN        2001-09-30   \n",
       "3 2002-01-08          True  SP-008644          NaN        2001-09-30   \n",
       "4 2002-01-08          True  SP-013994          NaN        2001-09-30   \n",
       "\n",
       "   E_TTM_ammor_intangibles  E_TTM_asset_writedown  E_TTM_assets_gro_five  \\\n",
       "0                    0.000                    0.0                    0.0   \n",
       "1                    3.078                    0.0                    0.0   \n",
       "2                    0.000                    0.0                    0.0   \n",
       "3                    0.000                    0.0                    0.0   \n",
       "4                    0.000                    0.0                    0.0   \n",
       "\n",
       "   E_TTM_capex  E_TTM_cash_acquisitions  ...  E_G_ebitda_cov  \\\n",
       "0   -12.738000                   -3.336  ...     -165.453488   \n",
       "1   -20.889000                  -68.220  ...       -2.685925   \n",
       "2   -17.971623                    0.000  ...        2.262460   \n",
       "3   -34.700000                    0.000  ...       -4.852273   \n",
       "4 -1403.000000                 -133.000  ...      -14.569697   \n",
       "\n",
       "   E_G_ret_on_asset  E_G_ret_on_inv_cap  E_G_net_to_cash  \\\n",
       "0          0.130018            0.101871        -0.068216   \n",
       "1          0.071119            0.067430        -0.004881   \n",
       "2         -0.069781           -0.039238        -0.045993   \n",
       "3         -0.169833           -0.155712        -0.316372   \n",
       "4          0.109798            0.078497        -0.157934   \n",
       "\n",
       "   E_G_perm_assets_ratio  return_1q  target_net_income  \\\n",
       "0               0.414230        NaN                NaN   \n",
       "1               0.595752        NaN                NaN   \n",
       "2               0.775432        NaN                NaN   \n",
       "3               0.773996        NaN                NaN   \n",
       "4               0.921832        NaN                NaN   \n",
       "\n",
       "   target_cash_operations  binary_target_net_income  \\\n",
       "0                     NaN                         0   \n",
       "1                     NaN                         0   \n",
       "2                     NaN                         0   \n",
       "3                     NaN                         0   \n",
       "4                     NaN                         0   \n",
       "\n",
       "   binary_target_cash_operations  \n",
       "0                              0  \n",
       "1                              0  \n",
       "2                              0  \n",
       "3                              0  \n",
       "4                              0  \n",
       "\n",
       "[5 rows x 670 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_canada = pd.read_csv('canada_updated.csv')\n",
    "df_canada[\"date\"] = pd.to_datetime(df_canada[\"date\"])\n",
    "df_canada.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copie pour modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_canada.copy()\n",
    "\n",
    "df_model['date'] = pd.to_datetime(df_model['date'], errors='coerce')\n",
    "df_model.sort_values(by=['cid', 'date'], inplace=True)\n",
    "\n",
    "# Retirer les lignes où Quality_Flag est False\n",
    "df_model = df_model[df_model['QUALITY_FLAG'] == True]\n",
    "\n",
    "# (FACULTATIF) Exclure les banques\n",
    "# df_model = df_model[df_model['industry'] != 'Banks']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(\n",
    "    df,\n",
    "    include_agro=False,\n",
    "    include_rgro=False,\n",
    "    include_tcgro=False,\n",
    "    include_ratios_assets=False,\n",
    "    include_ratios_rev=False,\n",
    "    include_ratios_totcap=False,\n",
    "    mandatory_cols=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Sélectionne dynamiquement les colonnes d'un DataFrame en fonction\n",
    "    des familles de variables explicatives demandées,\n",
    "    en plaçant d'abord les colonnes obligatoires (mandatory_cols).\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1) Gérer la liste mandatory_cols (par défaut : vide ou ['cid','date'] selon besoin)\n",
    "    if mandatory_cols is None:\n",
    "        mandatory_cols = []\n",
    "    \n",
    "    # 2) Définir les \"familles\" de motifs\n",
    "    family_patterns = {\n",
    "        'agro': ['_agro_1q', '_agro_4q'],\n",
    "        'rgro': ['_rgro_1q', '_rgro_4q'],\n",
    "        'tcgro': ['_tcgro_1q', '_tcgro_4q'],\n",
    "        'ratios_assets': ['_on_assets_ratio'],\n",
    "        'ratios_rev': ['_on_rev_ratio'],\n",
    "        'ratios_totcap': ['_on_tot_cap_ratio']\n",
    "    }\n",
    "    \n",
    "    # 3) Construire la liste des motifs à inclure\n",
    "    patterns_to_keep = []\n",
    "    if include_agro:\n",
    "        patterns_to_keep += family_patterns['agro']\n",
    "    if include_rgro:\n",
    "        patterns_to_keep += family_patterns['rgro']\n",
    "    if include_tcgro:\n",
    "        patterns_to_keep += family_patterns['tcgro']\n",
    "    \n",
    "    if include_ratios_assets:\n",
    "        patterns_to_keep += family_patterns['ratios_assets']\n",
    "    if include_ratios_rev:\n",
    "        patterns_to_keep += family_patterns['ratios_rev']\n",
    "    if include_ratios_totcap:\n",
    "        patterns_to_keep += family_patterns['ratios_totcap']\n",
    "    \n",
    "    # 4) Retrouver toutes les colonnes du df qui matchent nos motifs\n",
    "    matched_cols = set()\n",
    "    for pat in patterns_to_keep:\n",
    "        for col in df.columns:\n",
    "            if pat in col:\n",
    "                matched_cols.add(col)\n",
    "    # => matched_cols est un set() de colonnes\n",
    "    \n",
    "    # 5) Conserver l'ordre original des colonnes matched, \n",
    "    #    en filtrant df.columns dans l'ordre d'origine\n",
    "    matched_cols_in_order = [c for c in df.columns if c in matched_cols]\n",
    "    \n",
    "    # 6) Construire l'ordre final :\n",
    "    #    - d'abord mandatory_cols (dans l'ordre donné),\n",
    "    #    - puis les matched_cols (dans l'ordre d'origine)\n",
    "    #    - attention aux colonnes obligatoires qui n'existent pas, \n",
    "    #      ou aux duplications\n",
    "    #    - on fait donc une intersection + un set() pour éviter \n",
    "    #      les collisions.\n",
    "    \n",
    "    # Intersection pour ne pas inclure des mandatory inexistantes\n",
    "    mandatory_cols_in_df = [c for c in mandatory_cols if c in df.columns]\n",
    "    \n",
    "    # Puis on concatène en évitant toute duplication\n",
    "    columns_to_keep_ordered = mandatory_cols_in_df + [\n",
    "        c for c in matched_cols_in_order if c not in mandatory_cols_in_df\n",
    "    ]\n",
    "    \n",
    "    # 7) Extraire le sous-DataFrame\n",
    "    df_filtered = df[columns_to_keep_ordered].copy()\n",
    "    \n",
    "    # 8) (Optionnel) trier par cid/date si elles sont présentes\n",
    "    if 'cid' in df_filtered.columns and 'date' in df_filtered.columns:\n",
    "        df_filtered.sort_values(by=['cid', 'date'], inplace=True)\n",
    "    \n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appel de fonction - Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de valeurs NaN dans df_model_final : 1360861\n",
      "Nombre total de valeurs NaN après suppression : 0\n",
      "(36154, 579)\n"
     ]
    }
   ],
   "source": [
    "DF_ALL = select_features(\n",
    "    df_model,\n",
    "    include_agro=True,\n",
    "    include_rgro=True,\n",
    "    include_tcgro=True,\n",
    "    include_ratios_assets=True,\n",
    "    include_ratios_rev=True,\n",
    "    include_ratios_totcap=True,\n",
    "    mandatory_cols=['cid', 'date', 'binary_target_net_income']  # je garde la target\n",
    ")\n",
    "\n",
    "# Compter le nombre total de NaN dans tout le DataFrame\n",
    "total_nan = DF_ALL.isna().sum().sum()\n",
    "print(f\"Nombre total de valeurs NaN dans DF_ALL : {total_nan}\")\n",
    "\n",
    "# Retirer les lignes qui contiennent AU MOINS un NaN\n",
    "DF_ALL.dropna(inplace=True)\n",
    "\n",
    "# Vérifier à nouveau qu’il n’y a plus de NaN\n",
    "total_nan_apres = DF_ALL.isna().sum().sum()\n",
    "print(f\"Nombre total de valeurs NaN après suppression : {total_nan_apres}\")\n",
    "\n",
    "print(DF_ALL.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_Assets = select_features(\n",
    "    df_model,\n",
    "    include_agro=True,\n",
    "    include_rgro=False,\n",
    "    include_tcgro=False,\n",
    "    include_ratios_assets=True,\n",
    "    include_ratios_rev=False,\n",
    "    include_ratios_totcap=False,\n",
    "    mandatory_cols=['cid', 'date', 'binary_target_net_income']  # je garde la target\n",
    ")\n",
    "\n",
    "# Compter le nombre total de NaN dans tout le DataFrame\n",
    "total_nan = DF_Assets.isna().sum().sum()\n",
    "print(f\"Nombre total de valeurs NaN dans DF_Assets : {total_nan}\")\n",
    "\n",
    "# Retirer les lignes qui contiennent AU MOINS un NaN\n",
    "DF_Assets.dropna(inplace=True)\n",
    "\n",
    "# Vérifier à nouveau qu’il n’y a plus de NaN\n",
    "total_nan_apres = DF_Assets.isna().sum().sum()\n",
    "print(f\"Nombre total de valeurs NaN après suppression : {total_nan_apres}\")\n",
    "\n",
    "print(DF_Assets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revenues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_Revenues = select_features(\n",
    "    df_model,\n",
    "    include_agro=False,\n",
    "    include_rgro=True,\n",
    "    include_tcgro=False,\n",
    "    include_ratios_assets=False,\n",
    "    include_ratios_rev=True,\n",
    "    include_ratios_totcap=False,\n",
    "    mandatory_cols=['cid', 'date', 'binary_target_net_income']  # je garde la target\n",
    ")\n",
    "\n",
    "# Compter le nombre total de NaN dans tout le DataFrame\n",
    "total_nan = DF_Revenues.isna().sum().sum()\n",
    "print(f\"Nombre total de valeurs NaN dans DF_Revenues : {total_nan}\")\n",
    "\n",
    "# Retirer les lignes qui contiennent AU MOINS un NaN\n",
    "DF_Revenues.dropna(inplace=True)\n",
    "\n",
    "# Vérifier à nouveau qu’il n’y a plus de NaN\n",
    "total_nan_apres = DF_Revenues.isna().sum().sum()\n",
    "print(f\"Nombre total de valeurs NaN après suppression : {total_nan_apres}\")\n",
    "\n",
    "print(DF_Revenues.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capital Moyen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_Capital = select_features(\n",
    "    df_model,\n",
    "    include_agro=False,\n",
    "    include_rgro=False,\n",
    "    include_tcgro=True,\n",
    "    include_ratios_assets=False,\n",
    "    include_ratios_rev=False,\n",
    "    include_ratios_totcap=True,\n",
    "    mandatory_cols=['cid', 'date', 'binary_target_net_income']  # je garde la target\n",
    ")\n",
    "\n",
    "# Compter le nombre total de NaN dans tout le DataFrame\n",
    "total_nan = DF_Capital.isna().sum().sum()\n",
    "print(f\"Nombre total de valeurs NaN dans DF_Capital : {total_nan}\")\n",
    "\n",
    "# Retirer les lignes qui contiennent AU MOINS un NaN\n",
    "DF_Capital.dropna(inplace=True)\n",
    "\n",
    "# Vérifier à nouveau qu’il n’y a plus de NaN\n",
    "total_nan_apres = DF_Capital.isna().sum().sum()\n",
    "print(f\"Nombre total de valeurs NaN après suppression : {total_nan_apres}\")\n",
    "\n",
    "print(DF_Capital.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DF 10 ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-12-12 00:00:00 2024-12-12 00:00:00\n",
      "(15103, 579)\n"
     ]
    }
   ],
   "source": [
    "# Date la plus récente du DataFrame\n",
    "max_date = DF_ALL['date'].max()\n",
    "\n",
    "# Date de coupure (5 ans avant)\n",
    "cutoff_date = max_date - pd.DateOffset(years=10)\n",
    "\n",
    "# Filtrer pour ne garder que les 5 dernières années\n",
    "DF_10Y_all = DF_ALL[DF_ALL['date'] >= cutoff_date].copy()\n",
    "\n",
    "print(DF_10Y_all['date'].min(), DF_10Y_all['date'].max())\n",
    "print(DF_10Y_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date la plus récente du DataFrame\n",
    "max_date = DF_Assets['date'].max()\n",
    "\n",
    "# Date de coupure (5 ans avant)\n",
    "cutoff_date = max_date - pd.DateOffset(years=10)\n",
    "\n",
    "# Filtrer pour ne garder que les 5 dernières années\n",
    "DF_10Y_Assets = DF_Assets[DF_Assets['date'] >= cutoff_date].copy()\n",
    "\n",
    "print(DF_10Y_Assets['date'].min(), DF_10Y_Assets['date'].max())\n",
    "print(DF_10Y_Assets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revenues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date la plus récente du DataFrame\n",
    "max_date = DF_Revenues['date'].max()\n",
    "\n",
    "# Date de coupure (5 ans avant)\n",
    "cutoff_date = max_date - pd.DateOffset(years=10)\n",
    "\n",
    "# Filtrer pour ne garder que les 5 dernières années\n",
    "DF_10Y_Revenues = DF_Revenues[DF_Revenues['date'] >= cutoff_date].copy()\n",
    "\n",
    "print(DF_10Y_Revenues['date'].min(), DF_10Y_Revenues['date'].max())\n",
    "print(DF_10Y_Revenues.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capital Moyen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date la plus récente du DataFrame\n",
    "max_date = DF_Capital['date'].max()\n",
    "\n",
    "# Date de coupure (5 ans avant)\n",
    "cutoff_date = max_date - pd.DateOffset(years=10)\n",
    "\n",
    "# Filtrer pour ne garder que les 5 dernières années\n",
    "DF_10Y_Capital = DF_Capital[DF_Capital['date'] >= cutoff_date].copy()\n",
    "\n",
    "print(DF_10Y_Capital['date'].min(), DF_10Y_Capital['date'].max())\n",
    "print(DF_10Y_Capital.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Window + AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_rolling_windows(data, date_col, target_col, train_years, val_years, test_years, label, buffer_months=0):\n",
    "    \"\"\"\n",
    "    Pipeline direct pour la rolling window avec AutoML et cross-validation personnalisée.\n",
    "    Ajoute les périodes dans le DataFrame final pour validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Conversion de la colonne date\n",
    "    data[date_col] = pd.to_datetime(data[date_col])\n",
    "    start_date = data[date_col].min()\n",
    "    end_date = data[date_col].max()\n",
    "\n",
    "    predictions_all = []  # Liste pour stocker toutes les prédictions\n",
    "\n",
    "    while start_date + relativedelta(years=train_years + val_years + test_years) <= end_date:\n",
    "        # Définir les périodes\n",
    "        train_end = start_date + relativedelta(years=train_years) - pd.Timedelta(days=1)\n",
    "        tampon_1_end = train_end + relativedelta(months=buffer_months)\n",
    "        val_start = tampon_1_end + pd.Timedelta(days=1)\n",
    "        val_end = val_start + relativedelta(years=val_years) - pd.Timedelta(days=1)\n",
    "        tampon_2_end = val_end + relativedelta(months=buffer_months)\n",
    "        test_start = tampon_2_end + pd.Timedelta(days=1)\n",
    "        test_end = test_start + relativedelta(years=test_years) - pd.Timedelta(days=1)\n",
    "\n",
    "        # Filtrer les données\n",
    "        train_data = data.loc[(data[date_col] >= start_date) & (data[date_col] <= train_end)]\n",
    "        val_data = data.loc[(data[date_col] >= val_start) & (data[date_col] <= val_end)]\n",
    "        test_data = data.loc[(data[date_col] >= test_start) & (data[date_col] <= test_end)]\n",
    "\n",
    "        if len(train_data) == 0 or len(val_data) == 0 or len(test_data) == 0:\n",
    "            print(f\"Fenêtre {start_date.year}-{test_end.year} : données insuffisantes, sautée.\")\n",
    "            start_date += relativedelta(years=1)\n",
    "            continue\n",
    "\n",
    "        # Configurer et entraîner AutoML\n",
    "        print(f\"Fenêtre {start_date.year}-{test_end.year} : entraînement de AutoML...\")\n",
    "        automl = AutoML(results_path=f\"AutoML_{start_date.year}-{test_end.year}_{label}\", mode=\"Perform\", algorithms=[\"Xgboost\"], eval_metric=\"auc\")\n",
    "        custom_cv = [(train_data.index, val_data.index)]\n",
    "        automl.fit(\n",
    "            train_data.drop(columns=[target_col, date_col, 'cid']),\n",
    "            train_data[target_col], cv=custom_cv\n",
    "        )\n",
    "\n",
    "        # Prédire sur le test set\n",
    "        test_preds = test_data[[date_col, target_col]].copy()\n",
    "        #test_preds[\"predicted\"] = automl.predict_proba(test_data.drop(columns=[target_col, date_col, 'cid']))\n",
    "        proba = automl.predict_proba(test_data.drop(columns=[target_col, date_col, 'cid']))\n",
    "        test_preds[\"prob_down\"] = proba[:, 0]  # Probabilité pour la classe 0 (diminution des bénéfices)\n",
    "        test_preds[\"prob_up\"] = proba[:, 1]  # Probabilité pour la classe 1 (augmentation des bénéfices)\n",
    "        test_preds[\"net_prob_score\"] = test_preds[\"prob_up\"] - test_preds[\"prob_down\"]\n",
    "        test_preds[\"window\"] = f\"{start_date.year}-{test_end.year}\"\n",
    "        test_preds[\"cid\"] = test_data[\"cid\"].values\n",
    "\n",
    "        # Ajouter les périodes pour validation\n",
    "        #test_preds[\"train_start\"] = start_date\n",
    "        #test_preds[\"train_end\"] = train_end\n",
    "        #test_preds[\"tampon_1\"] = tampon_1_end\n",
    "        #test_preds[\"val_start\"] = val_start\n",
    "        #test_preds[\"val_end\"] = val_end\n",
    "        #test_preds[\"tampon_2\"] = tampon_2_end\n",
    "        #test_preds[\"test_start\"] = test_start\n",
    "        #test_preds[\"test_end\"] = test_end\n",
    "\n",
    "        # Sauvegarder les prédictions\n",
    "        predictions_all.append(test_preds)\n",
    "\n",
    "        # Avancer la fenêtre\n",
    "        start_date += relativedelta(years=1)\n",
    "\n",
    "    predictions_df = pd.concat(predictions_all, ignore_index=True)\n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appel de fonction - Prédictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10Y ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2583970802.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    data=DF_10Y_all,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#predictions_df_all = pipeline_rolling_windows(\n",
    "    #data=DF_10Y_all,\n",
    "    #date_col=\"date\", \n",
    "    #target_col=\"binary_target_net_income\", \n",
    "    #train_years=5, \n",
    "    #val_years=1, \n",
    "    #test_years=1,\n",
    "    #label=\"all\", \n",
    "    #buffer_months=1\n",
    "#)\n",
    "\n",
    "#predictions_df_all.to_csv(\"df10Y_NetInc_Pred_all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10Y Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df_assets = pipeline_rolling_windows(\n",
    "    data=DF_10Y_Assets,\n",
    "    date_col=\"date\", \n",
    "    target_col=\"binary_target_net_income\", \n",
    "    train_years=5, \n",
    "    val_years=1, \n",
    "    test_years=1,\n",
    "    label=\"assets\", \n",
    "    buffer_months=1\n",
    ")\n",
    "\n",
    "predictions_df_assets.to_csv(\"df10Y_NetInc_Pred_assets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10Y Revenues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df_revenues = pipeline_rolling_windows(\n",
    "    data=DF_10Y_Revenues,\n",
    "    date_col=\"date\", \n",
    "    target_col=\"binary_target_net_income\", \n",
    "    train_years=5, \n",
    "    val_years=1, \n",
    "    test_years=1,\n",
    "    label=\"Revenue\", \n",
    "    buffer_months=1\n",
    ")\n",
    "\n",
    "predictions_df_revenues.to_csv(\"df10Y_NetInc_Pred_revenues.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10Y Capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df_capital = pipeline_rolling_windows(\n",
    "    data=DF_10Y_Capital,\n",
    "    date_col=\"date\", \n",
    "    target_col=\"binary_target_net_income\", \n",
    "    train_years=5, \n",
    "    val_years=1, \n",
    "    test_years=1,\n",
    "    label=\"capital\", \n",
    "    buffer_months=1\n",
    ")\n",
    "\n",
    "predictions_df_capital.to_csv(\"df10Y_NetInc_Pred_capital.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC - Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6703369325280739, 0.6703368649485612, 0.3296630674719261)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test_auc_net = roc_auc_score(predictions_df[\"binary_target_net_income\"], predictions_df[\"net_prob_score\"])\n",
    "#test_auc_up = roc_auc_score(predictions_df[\"binary_target_net_income\"], predictions_df[\"prob_up\"])\n",
    "#test_auc_down = roc_auc_score(predictions_df[\"binary_target_net_income\"], predictions_df[\"prob_down\"])\n",
    "#test_auc_net, test_auc_up, test_auc_down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped AUCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_auc_by_groups(\n",
    "    predictions_df: pd.DataFrame, \n",
    "    df_canada: pd.DataFrame,\n",
    "    group_cols: list,\n",
    "    cid_col: str = \"cid\",\n",
    "    date_col: str = \"date\",\n",
    "    target_col: str = \"binary_target_net_income\",\n",
    "    prob_col: str = \"prob_up\",\n",
    "    out_csv: str = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Calcule l'AUC par groupes (selon group_cols) et le nombre de lignes dans chaque groupe.\n",
    "    \n",
    "    Arguments :\n",
    "    -----------\n",
    "    - predictions_df : DataFrame contenant au moins [cid_col, date_col, target_col, prob_col]\n",
    "    - df_canada : DataFrame contenant au moins [cid_col, date_col, 'return_1q', 'industry_raw', etc.]\n",
    "    - group_cols : liste des colonnes sur lesquelles faire le groupby (ex: [\"year\", \"industry_raw\"])\n",
    "    - cid_col : nom de la colonne identifiant l'entreprise (défaut: \"cid\")\n",
    "    - date_col : nom de la colonne date (défaut: \"date\")\n",
    "    - target_col : nom de la colonne cible binaire (ex: 0/1) (défaut: \"binary_target_net_income\")\n",
    "    - prob_col : nom de la colonne contenant la probabilité pour la classe positive (défaut: \"prob_up\")\n",
    "    - out_csv : chemin vers le fichier CSV de sortie (défaut: None, ne pas exporter)\n",
    "\n",
    "    Retour :\n",
    "    --------\n",
    "    - final_df : DataFrame avec AUC et Count pour chaque groupe\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Fusionner les dataframes (left merge pour conserver toutes les prédictions)\n",
    "    df_merged = predictions_df.merge(\n",
    "        df_canada[[cid_col, date_col, 'return_1q', 'industry_raw']],  # adapt si tu veux plus/moins de colonnes\n",
    "        on=[cid_col, date_col],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # 2) Si \"year\" est dans group_cols et pas encore créée, on l'extrait du date_col\n",
    "    if \"year\" in group_cols and \"year\" not in df_merged.columns:\n",
    "        df_merged[\"year\"] = pd.to_datetime(df_merged[date_col]).dt.year\n",
    "\n",
    "    # -- Fonction interne pour calculer l’AUC sur un sous-groupe\n",
    "    def group_auc(sub_df):\n",
    "        unique_targets = sub_df[target_col].unique()\n",
    "        # Gérer le cas où il n’y a qu’une seule classe (roc_auc_score plantait sinon)\n",
    "        if len(unique_targets) < 2:\n",
    "            return float(\"nan\")\n",
    "        return roc_auc_score(sub_df[target_col], sub_df[prob_col])\n",
    "\n",
    "    # 3) Calculer l’AUC par groupe\n",
    "    grouped_aucs = (\n",
    "        df_merged\n",
    "        .groupby(group_cols)\n",
    "        .apply(group_auc)\n",
    "        .reset_index(name=\"AUC\")\n",
    "    )\n",
    "\n",
    "    # 4) Calculer le nombre d’observations par groupe (ou nombre de cid uniques, selon besoin)\n",
    "    #    Ici : on compte simplement le nombre de lignes\n",
    "    grouped_counts = (\n",
    "        df_merged\n",
    "        .groupby(group_cols)[target_col]  # ou [cid_col] si tu veux le nombre d'entreprises distinctes\n",
    "        .count()\n",
    "        .reset_index(name=\"Count\")\n",
    "    )\n",
    "\n",
    "    # 5) Fusionner AUC et Count\n",
    "    final_df = pd.merge(grouped_aucs, grouped_counts, on=group_cols, how=\"left\")\n",
    "\n",
    "    # Optionnel : trier le résultat\n",
    "    final_df = final_df.sort_values(group_cols)\n",
    "\n",
    "    # Optionnel : exporter en CSV\n",
    "    if out_csv is not None:\n",
    "        final_df.to_csv(out_csv, index=False)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appel de Fonction - Grouped AUCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ns/nlrkcsjd7j70p9jvf2jl5t4c0000gn/T/ipykernel_1371/2025299659.py:53: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021</td>\n",
       "      <td>0.707484</td>\n",
       "      <td>1414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022</td>\n",
       "      <td>0.707988</td>\n",
       "      <td>1491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.744579</td>\n",
       "      <td>1457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year       AUC  Count\n",
       "0  2021  0.707484   1414\n",
       "1  2022  0.707988   1491\n",
       "2  2023  0.744579   1457\n",
       "3  2024       NaN   1386"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute_auc_by_groups(\n",
    "    #predictions_df=predictions_df_all,\n",
    "    #df_canada=df_canada,\n",
    "    #group_cols=[\"year\"],\n",
    "    #out_csv=\"DF10Y_NetInc_AUC_Year_all.csv\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_auc_by_groups(\n",
    "    predictions_df=predictions_df_assets,\n",
    "    df_canada=df_canada,\n",
    "    group_cols=[\"year\"],\n",
    "    out_csv=\"DF10Y_NetInc_AUC_Year_assets.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revenues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_auc_by_groups(\n",
    "    predictions_df=predictions_df_revenues,\n",
    "    df_canada=df_canada,\n",
    "    group_cols=[\"year\"],\n",
    "    out_csv=\"DF10Y_NetInc_AUC_Year_revenues.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_auc_by_groups(\n",
    "    predictions_df=predictions_df_capital,\n",
    "    df_canada=df_canada,\n",
    "    group_cols=[\"year\"],\n",
    "    out_csv=\"DF10Y_NetInc_AUC_Year_capital.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création de Portefeuille + Annual Weighted Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weighted_portfolios(predictions_df, df_canada, proba_col, return_col, lower_threshold=0.4, upper_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Crée un portefeuille pondéré basé sur les prédictions et calcule les rendements pondérés par année,\n",
    "    en attribuant des poids positifs aux positions longues et négatifs aux positions courtes, avec une somme neutre.\n",
    "    \n",
    "    Args:\n",
    "    - predictions_df (pd.DataFrame): DataFrame contenant les prédictions et les identifiants 'cid'.\n",
    "    - df_canada (pd.DataFrame): DataFrame contenant les rendements futurs et les identifiants 'cid'.\n",
    "    - return_col (str): Nom de la colonne des rendements futurs dans df_canada.\n",
    "    - lower_threshold (float): Seuil inférieur pour les positions courtes.\n",
    "    - upper_threshold (float): Seuil supérieur pour les positions longues.\n",
    "    \n",
    "    Returns:\n",
    "    - result_df (pd.DataFrame): DataFrame contenant les rendements pondérés des portefeuilles par année.\n",
    "    \"\"\"\n",
    "    # Assure que les colonnes 'date' sont au bon format datetime dans les deux DataFrames\n",
    "    predictions_df['date'] = pd.to_datetime(predictions_df['date'])\n",
    "    df_canada['date'] = pd.to_datetime(df_canada['date'])\n",
    "        \n",
    "    \n",
    "    # Joindre les deux DataFrames sur 'cid' et 'date'\n",
    "    merged_df = predictions_df.merge(df_canada[['cid', 'date', return_col]], on=['cid', 'date'], how='left')\n",
    "    \n",
    "    # Extraire l'année à partir de la colonne 'date'\n",
    "    merged_df['year'] = merged_df['date'].dt.year\n",
    "    \n",
    "    # Retirer les lignes avec des valeurs manquantes\n",
    "    merged_df.dropna(inplace=True)\n",
    "    \n",
    "    # Initialiser une liste pour stocker les résultats\n",
    "    results = []\n",
    "\n",
    "    # Grouper par année\n",
    "    for year, group in merged_df.groupby('year'):\n",
    "        # Sélectionner les positions longues et courtes selon les seuils\n",
    "        selected_long = group[group[proba_col] > upper_threshold]  # Positions longues\n",
    "        selected_short = group[group[proba_col] < lower_threshold]  # Positions courtes\n",
    "        \n",
    "        n_long = len(selected_long)\n",
    "        n_short = len(selected_short)\n",
    "        \n",
    "        if n_long > 0 or n_short > 0:\n",
    "            # Attribuer des poids aux positions longues et courtes\n",
    "            if n_long > 0:\n",
    "                selected_long.loc[:, 'weight'] = 1 / n_long  # Poids positifs pour les positions longues\n",
    "            if n_short > 0:\n",
    "                selected_short.loc[:, 'weight'] = -1 / n_short  # Poids négatifs pour les positions courtes\n",
    "            \n",
    "            # Combiner les deux DataFrames\n",
    "            selected = pd.concat([selected_long, selected_short], ignore_index=True)\n",
    "            \n",
    "            # Calculer le rendement pondéré du portefeuille (somme des rendements pondérés)\n",
    "            weighted_return = (selected['weight'] * selected[return_col] * 100).sum()\n",
    "            \n",
    "            # Ajouter le résultat à la liste\n",
    "            results.append({'year': year, 'weighted_return': weighted_return})\n",
    "        else:\n",
    "            # Si aucune action ne respecte les seuils, le rendement est NaN\n",
    "            results.append({'year': year, 'weighted_return': float('nan')})\n",
    "\n",
    "    # Convertir les résultats en DataFrame\n",
    "    result_df = pd.DataFrame(results)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appel de fonction rendement annuel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ns/nlrkcsjd7j70p9jvf2jl5t4c0000gn/T/ipykernel_1371/444120012.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/var/folders/ns/nlrkcsjd7j70p9jvf2jl5t4c0000gn/T/ipykernel_1371/444120012.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/var/folders/ns/nlrkcsjd7j70p9jvf2jl5t4c0000gn/T/ipykernel_1371/444120012.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/var/folders/ns/nlrkcsjd7j70p9jvf2jl5t4c0000gn/T/ipykernel_1371/444120012.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/var/folders/ns/nlrkcsjd7j70p9jvf2jl5t4c0000gn/T/ipykernel_1371/444120012.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/var/folders/ns/nlrkcsjd7j70p9jvf2jl5t4c0000gn/T/ipykernel_1371/444120012.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/var/folders/ns/nlrkcsjd7j70p9jvf2jl5t4c0000gn/T/ipykernel_1371/444120012.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/var/folders/ns/nlrkcsjd7j70p9jvf2jl5t4c0000gn/T/ipykernel_1371/444120012.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>weighted_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021</td>\n",
       "      <td>4.214508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022</td>\n",
       "      <td>0.797524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.032612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024</td>\n",
       "      <td>-2.644895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  weighted_return\n",
       "0  2021         4.214508\n",
       "1  2022         0.797524\n",
       "2  2023         0.032612\n",
       "3  2024        -2.644895"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ret_net_income_all = create_weighted_portfolios(predictions_df_all, df_canada, 'prob_up', 'return_1q', lower_threshold=0.5, upper_threshold=0.5)\n",
    "\n",
    "#ret_net_income.to_csv(\"returns_net_income_0.5_0.5.csv\", index=False)\n",
    "\n",
    "#ret_net_income_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ns/nlrkcsjd7j70p9jvf2jl5t4c0000gn/T/ipykernel_1371/444120012.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/var/folders/ns/nlrkcsjd7j70p9jvf2jl5t4c0000gn/T/ipykernel_1371/444120012.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/var/folders/ns/nlrkcsjd7j70p9jvf2jl5t4c0000gn/T/ipykernel_1371/444120012.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/var/folders/ns/nlrkcsjd7j70p9jvf2jl5t4c0000gn/T/ipykernel_1371/444120012.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/var/folders/ns/nlrkcsjd7j70p9jvf2jl5t4c0000gn/T/ipykernel_1371/444120012.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/var/folders/ns/nlrkcsjd7j70p9jvf2jl5t4c0000gn/T/ipykernel_1371/444120012.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/var/folders/ns/nlrkcsjd7j70p9jvf2jl5t4c0000gn/T/ipykernel_1371/444120012.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/var/folders/ns/nlrkcsjd7j70p9jvf2jl5t4c0000gn/T/ipykernel_1371/444120012.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>weighted_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021</td>\n",
       "      <td>4.378128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022</td>\n",
       "      <td>0.891550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.051322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024</td>\n",
       "      <td>-2.973748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  weighted_return\n",
       "0  2021         4.378128\n",
       "1  2022         0.891550\n",
       "2  2023         0.051322\n",
       "3  2024        -2.973748"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#returns_net_income_all = create_weighted_portfolios(predictions_df_all, df_canada, 'prob_up', 'return_1q', lower_threshold=0.4, upper_threshold=0.6)\n",
    "\n",
    "#returns_net_income.to_csv(\"returns_net_income_0.4_0.6.csv\", index=False)\n",
    "\n",
    "#returns_net_income_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendement par Décile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_decile_returns(\n",
    "    predictions_df: pd.DataFrame,\n",
    "    df_canada: pd.DataFrame,\n",
    "    proba_col: str,\n",
    "    return_col: str,\n",
    "    aggregator: str = \"mean\",        # \"mean\" ou \"median\"\n",
    "    decile_weights: list = None,     # liste de 10 poids ou None\n",
    "    csv_path: str = None             # chemin CSV ou None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calcule le rendement par décile (du plus faible au plus élevé) pour chaque année.\n",
    "    - Découpage en déciles via pd.qcut (q=10).\n",
    "    - Agrégation des rendements par décile via un 'aggregator' (mean ou median).\n",
    "    - Optionnellement, applique un vecteur de poids (decile_weights) pour calculer\n",
    "      un rendement global (somme pondérée des déciles).\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    predictions_df : pd.DataFrame\n",
    "        Contient les prédictions (probabilités) et les colonnes 'cid', 'date'.\n",
    "        Doit inclure la colonne `proba_col`.\n",
    "    df_canada : pd.DataFrame\n",
    "        Contient la colonne `return_col` ainsi que 'cid', 'date'.\n",
    "    proba_col : str\n",
    "        Nom de la colonne dans predictions_df représentant la probabilité ou le score.\n",
    "    return_col : str\n",
    "        Nom de la colonne dans df_canada qui contient le rendement (ou variation).\n",
    "    aggregator : {\"mean\", \"median\"}, défaut = \"mean\"\n",
    "        Choix entre la moyenne ou la médiane comme statistique de rendement pour le décile.\n",
    "    decile_weights : list of float (length 10) ou None\n",
    "        Si fourni, liste de 10 poids (un pour chaque décile de 0 à 9).\n",
    "        Permet de calculer une colonne \"portfolio_return\" = somme(décile_i * poids_i).\n",
    "    csv_path : str ou None\n",
    "        Si un chemin est fourni, le DataFrame final sera sauvegardé en CSV.\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    df_final : pd.DataFrame\n",
    "        DataFrame pivoté avec:\n",
    "        - 1 ligne par année\n",
    "        - 10 colonnes (D1 à D10) indiquant le rendement agrégé de chaque décile\n",
    "        - Si decile_weights est fourni, une colonne supplémentaire 'portfolio_return'\n",
    "          représentant la somme pondérée des rendements déciles.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1) Merge sur cid + date\n",
    "    merged_df = predictions_df.merge(\n",
    "        df_canada[['cid', 'date', return_col]],\n",
    "        on=['cid', 'date'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # --- 2) Extraire l'année\n",
    "    merged_df['year'] = merged_df['date'].dt.year\n",
    "    \n",
    "    # --- 3) Nettoyage éventuel\n",
    "    merged_df.dropna(subset=[proba_col, return_col, 'year'], inplace=True)\n",
    "    \n",
    "    # --- 4) Choix de la fonction d'agrégation\n",
    "    if aggregator == \"mean\":\n",
    "        aggregator_func = np.mean\n",
    "    elif aggregator == \"median\":\n",
    "        aggregator_func = np.median\n",
    "    else:\n",
    "        raise ValueError(\"aggregator must be 'mean' or 'median'.\")\n",
    "    \n",
    "    # --- 5) Préparer une liste pour stocker (year, decile_bin, agg_return)\n",
    "    decile_results = []\n",
    "    \n",
    "    for year, group in merged_df.groupby('year'):\n",
    "        # On doit découper en 10 déciles\n",
    "        group = group.copy()\n",
    "        \n",
    "        # Astuce: labels=False renvoie des labels 0..9\n",
    "        group['decile_bin'] = pd.qcut(group[proba_col], q=10, labels=False)\n",
    "        \n",
    "        # Calculer l'agrégat (mean ou median) du rendement dans chaque décile\n",
    "        for decile_id, subdf in group.groupby('decile_bin'):\n",
    "            if pd.isnull(decile_id):\n",
    "                continue\n",
    "            \n",
    "            decile_id = int(decile_id)\n",
    "            # Rendement agrégé (en % si on veut)\n",
    "            decile_value = aggregator_func(subdf[return_col]) * 100.0\n",
    "            \n",
    "            decile_results.append({\n",
    "                'year': year,\n",
    "                'decile': decile_id,\n",
    "                'decile_return': decile_value\n",
    "            })\n",
    "    \n",
    "    # --- 6) On transforme la liste en DataFrame + pivot\n",
    "    decile_df = pd.DataFrame(decile_results)\n",
    "    df_pivot = decile_df.pivot(index='year', columns='decile', values='decile_return')\n",
    "    \n",
    "    # Renommer les colonnes pour D1..D10\n",
    "    # Attention : decile_id = 0 correspond au 1er décile => D1\n",
    "    df_pivot.columns = [f\"D{i+1}\" for i in df_pivot.columns]\n",
    "    \n",
    "    # --- 7) Si on a des poids, on calcule la somme pondérée\n",
    "    if decile_weights is not None:\n",
    "        if len(decile_weights) != 10:\n",
    "            raise ValueError(\"decile_weights must be a list of length 10.\")\n",
    "        \n",
    "        # Multiplier chaque colonne D1..D10 par le poids correspondant, puis sommer\n",
    "        # Astuce : df_pivot.iloc[:, 0:10] prend les 10 premières colonnes => D1..D10\n",
    "        # On peut ensuite .mul(decile_weights, axis='columns') pour multiplier par le vecteur\n",
    "        # et .sum(axis=1) pour obtenir la somme par année\n",
    "        # (on suppose que l'ordre des colonnes est D1..D10).\n",
    "        \n",
    "        weighted_sum = df_pivot.iloc[:, 0:10].mul(decile_weights, axis='columns').sum(axis=1)\n",
    "        \n",
    "        # On l'ajoute comme nouvelle colonne\n",
    "        df_pivot['portfolio_return'] = weighted_sum\n",
    "    \n",
    "    # --- 8) On remet l'index 'year' comme colonne\n",
    "    df_final = df_pivot.reset_index()\n",
    "    \n",
    "    # --- 9) Sauvegarde CSV si demandé\n",
    "    if csv_path is not None:\n",
    "        df_final.to_csv(csv_path, index=False)\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appel de fonction - Rendement par Décile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decile Weights : [-0.25, -0.25, -0.25, -0.25, 0, 0, 0.25, 0.25, 0.25, 0.25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>D10</th>\n",
       "      <th>portfolio_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021</td>\n",
       "      <td>4.606986</td>\n",
       "      <td>2.113403</td>\n",
       "      <td>4.389568</td>\n",
       "      <td>5.358879</td>\n",
       "      <td>8.193158</td>\n",
       "      <td>5.397031</td>\n",
       "      <td>7.390472</td>\n",
       "      <td>4.201719</td>\n",
       "      <td>9.778015</td>\n",
       "      <td>15.064996</td>\n",
       "      <td>4.991591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022</td>\n",
       "      <td>2.671292</td>\n",
       "      <td>0.847816</td>\n",
       "      <td>-3.298129</td>\n",
       "      <td>-3.150311</td>\n",
       "      <td>-3.835077</td>\n",
       "      <td>-3.998812</td>\n",
       "      <td>-3.531703</td>\n",
       "      <td>-4.570153</td>\n",
       "      <td>-0.614925</td>\n",
       "      <td>1.762166</td>\n",
       "      <td>-1.006321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.916306</td>\n",
       "      <td>0.946266</td>\n",
       "      <td>0.816035</td>\n",
       "      <td>4.053523</td>\n",
       "      <td>3.450989</td>\n",
       "      <td>3.270271</td>\n",
       "      <td>1.951572</td>\n",
       "      <td>6.181214</td>\n",
       "      <td>0.803168</td>\n",
       "      <td>0.962816</td>\n",
       "      <td>0.791659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024</td>\n",
       "      <td>4.630117</td>\n",
       "      <td>3.786070</td>\n",
       "      <td>8.280142</td>\n",
       "      <td>5.230720</td>\n",
       "      <td>4.432093</td>\n",
       "      <td>4.134712</td>\n",
       "      <td>4.108770</td>\n",
       "      <td>2.093775</td>\n",
       "      <td>0.884630</td>\n",
       "      <td>3.045103</td>\n",
       "      <td>-2.948693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year        D1        D2        D3        D4        D5        D6        D7  \\\n",
       "0  2021  4.606986  2.113403  4.389568  5.358879  8.193158  5.397031  7.390472   \n",
       "1  2022  2.671292  0.847816 -3.298129 -3.150311 -3.835077 -3.998812 -3.531703   \n",
       "2  2023  0.916306  0.946266  0.816035  4.053523  3.450989  3.270271  1.951572   \n",
       "3  2024  4.630117  3.786070  8.280142  5.230720  4.432093  4.134712  4.108770   \n",
       "\n",
       "         D8        D9        D10  portfolio_return  \n",
       "0  4.201719  9.778015  15.064996          4.991591  \n",
       "1 -4.570153 -0.614925   1.762166         -1.006321  \n",
       "2  6.181214  0.803168   0.962816          0.791659  \n",
       "3  2.093775  0.884630   3.045103         -2.948693  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_deciles_all = compute_decile_returns(\n",
    "    #predictions_df=predictions_df_all,\n",
    "    #df_canada=df_canada,\n",
    "    #proba_col='prob_up',\n",
    "    #return_col='return_1q',\n",
    "    #aggregator='mean',\n",
    "    #decile_weights=[-0.25, -0.25, -0.25, -0.25, 0, 0, 0.25, 0.25, 0.25, 0.25],\n",
    "    #csv_path='DF10Y_NetInc_decile_returns_all.csv'\n",
    "#)\n",
    "\n",
    "#df_deciles_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deciles_assets = compute_decile_returns(\n",
    "    predictions_df=predictions_df_assets,\n",
    "    df_canada=df_canada,\n",
    "    proba_col='prob_up',\n",
    "    return_col='return_1q',\n",
    "    aggregator='mean',\n",
    "    decile_weights=[-0.25, -0.25, -0.25, -0.25, 0, 0, 0.25, 0.25, 0.25, 0.25],\n",
    "    csv_path='DF10Y_NetInc_decile_returns_assets.csv'\n",
    ")\n",
    "\n",
    "df_deciles_assets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revenues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deciles_revenues = compute_decile_returns(\n",
    "    predictions_df=predictions_df_revenues,\n",
    "    df_canada=df_canada,\n",
    "    proba_col='prob_up',\n",
    "    return_col='return_1q',\n",
    "    aggregator='mean',\n",
    "    decile_weights=[-0.25, -0.25, -0.25, -0.25, 0, 0, 0.25, 0.25, 0.25, 0.25],\n",
    "    csv_path='DF10Y_NetInc_decile_returns_revenues.csv'\n",
    ")\n",
    "\n",
    "df_deciles_revenues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deciles_capital = compute_decile_returns(\n",
    "    predictions_df=predictions_df_capital,\n",
    "    df_canada=df_canada,\n",
    "    proba_col='prob_up',\n",
    "    return_col='return_1q',\n",
    "    aggregator='mean',\n",
    "    decile_weights=[-0.25, -0.25, -0.25, -0.25, 0, 0, 0.25, 0.25, 0.25, 0.25],\n",
    "    csv_path='DF10Y_NetInc_decile_returns_capital.csv'\n",
    ")\n",
    "\n",
    "df_deciles_revenues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decile Weights : [-0.20, -0.20, -0.20, -0.20, -0.20, 0.20, 0.20, 0.20, 0.20, 0.20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preuve CSV Rendement par Décile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- 1) Merge sur cid + date\n",
    "merged_df = predictions_df.merge(\n",
    "    df_canada[['cid', 'date', 'return_1q']],\n",
    "    on=['cid', 'date'],\n",
    "    how='left'\n",
    "    )\n",
    "\n",
    "# --- 2) Extraire l'année\n",
    "merged_df['year'] = merged_df['date'].dt.year\n",
    "\n",
    "# Filtrer pour l'année 2023\n",
    "year = 2023\n",
    "year_df = merged_df[merged_df['year'] == year].copy()\n",
    "\n",
    "# Attribuer les déciles pour cette année\n",
    "year_df['decile_bin'] = pd.qcut(year_df['prob_up'], q=10, labels=False)\n",
    "\n",
    "# Liste pour stocker chaque subdf avec une colonne indiquant le décile\n",
    "list_of_subdfs = []\n",
    "\n",
    "# Itérer sur chaque décile et collecter les sous-ensembles\n",
    "for decile_id, subdf in year_df.groupby('decile_bin'):\n",
    "    # Ajouter une colonne indiquant le décile courant\n",
    "    subdf = subdf.copy()\n",
    "    subdf['decile_id'] = decile_id\n",
    "    list_of_subdfs.append(subdf)\n",
    "\n",
    "# Concaténer tous les subdf en un seul DataFrame\n",
    "all_subdfs_df = pd.concat(list_of_subdfs, ignore_index=True)\n",
    "\n",
    "# Exporter vers CSV\n",
    "#all_subdfs_df.to_csv(\"subdfs_2023.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HECFinance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
