{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from supervised.automl import AutoML\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DF Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>QUALITY_FLAG</th>\n",
       "      <th>cid</th>\n",
       "      <th>industry_raw</th>\n",
       "      <th>E_TTM_period_date</th>\n",
       "      <th>E_TTM_ammor_intangibles</th>\n",
       "      <th>E_TTM_asset_writedown</th>\n",
       "      <th>E_TTM_assets_gro_five</th>\n",
       "      <th>E_TTM_capex</th>\n",
       "      <th>E_TTM_cash_acquisitions</th>\n",
       "      <th>...</th>\n",
       "      <th>E_G_ebitda_cov</th>\n",
       "      <th>E_G_ret_on_asset</th>\n",
       "      <th>E_G_ret_on_inv_cap</th>\n",
       "      <th>E_G_net_to_cash</th>\n",
       "      <th>E_G_perm_assets_ratio</th>\n",
       "      <th>return_1q</th>\n",
       "      <th>target_net_income</th>\n",
       "      <th>target_cash_operations</th>\n",
       "      <th>binary_target_net_income</th>\n",
       "      <th>binary_target_cash_operations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2002-01-03</td>\n",
       "      <td>True</td>\n",
       "      <td>SP-065996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001-10-31</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.738000</td>\n",
       "      <td>-3.336</td>\n",
       "      <td>...</td>\n",
       "      <td>-165.453488</td>\n",
       "      <td>0.130018</td>\n",
       "      <td>0.101871</td>\n",
       "      <td>-0.068216</td>\n",
       "      <td>0.414230</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2002-01-08</td>\n",
       "      <td>True</td>\n",
       "      <td>SP-002396</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001-09-30</td>\n",
       "      <td>3.078</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.889000</td>\n",
       "      <td>-68.220</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.685925</td>\n",
       "      <td>0.071119</td>\n",
       "      <td>0.067430</td>\n",
       "      <td>-0.004881</td>\n",
       "      <td>0.595752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002-01-08</td>\n",
       "      <td>True</td>\n",
       "      <td>SP-006704</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001-09-30</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-17.971623</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.262460</td>\n",
       "      <td>-0.069781</td>\n",
       "      <td>-0.039238</td>\n",
       "      <td>-0.045993</td>\n",
       "      <td>0.775432</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2002-01-08</td>\n",
       "      <td>True</td>\n",
       "      <td>SP-008644</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001-09-30</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-34.700000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.852273</td>\n",
       "      <td>-0.169833</td>\n",
       "      <td>-0.155712</td>\n",
       "      <td>-0.316372</td>\n",
       "      <td>0.773996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2002-01-08</td>\n",
       "      <td>True</td>\n",
       "      <td>SP-013994</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001-09-30</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1403.000000</td>\n",
       "      <td>-133.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.569697</td>\n",
       "      <td>0.109798</td>\n",
       "      <td>0.078497</td>\n",
       "      <td>-0.157934</td>\n",
       "      <td>0.921832</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 670 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  QUALITY_FLAG        cid industry_raw E_TTM_period_date  \\\n",
       "0 2002-01-03          True  SP-065996          NaN        2001-10-31   \n",
       "1 2002-01-08          True  SP-002396          NaN        2001-09-30   \n",
       "2 2002-01-08          True  SP-006704          NaN        2001-09-30   \n",
       "3 2002-01-08          True  SP-008644          NaN        2001-09-30   \n",
       "4 2002-01-08          True  SP-013994          NaN        2001-09-30   \n",
       "\n",
       "   E_TTM_ammor_intangibles  E_TTM_asset_writedown  E_TTM_assets_gro_five  \\\n",
       "0                    0.000                    0.0                    0.0   \n",
       "1                    3.078                    0.0                    0.0   \n",
       "2                    0.000                    0.0                    0.0   \n",
       "3                    0.000                    0.0                    0.0   \n",
       "4                    0.000                    0.0                    0.0   \n",
       "\n",
       "   E_TTM_capex  E_TTM_cash_acquisitions  ...  E_G_ebitda_cov  \\\n",
       "0   -12.738000                   -3.336  ...     -165.453488   \n",
       "1   -20.889000                  -68.220  ...       -2.685925   \n",
       "2   -17.971623                    0.000  ...        2.262460   \n",
       "3   -34.700000                    0.000  ...       -4.852273   \n",
       "4 -1403.000000                 -133.000  ...      -14.569697   \n",
       "\n",
       "   E_G_ret_on_asset  E_G_ret_on_inv_cap  E_G_net_to_cash  \\\n",
       "0          0.130018            0.101871        -0.068216   \n",
       "1          0.071119            0.067430        -0.004881   \n",
       "2         -0.069781           -0.039238        -0.045993   \n",
       "3         -0.169833           -0.155712        -0.316372   \n",
       "4          0.109798            0.078497        -0.157934   \n",
       "\n",
       "   E_G_perm_assets_ratio  return_1q  target_net_income  \\\n",
       "0               0.414230        NaN                NaN   \n",
       "1               0.595752        NaN                NaN   \n",
       "2               0.775432        NaN                NaN   \n",
       "3               0.773996        NaN                NaN   \n",
       "4               0.921832        NaN                NaN   \n",
       "\n",
       "   target_cash_operations  binary_target_net_income  \\\n",
       "0                     NaN                         0   \n",
       "1                     NaN                         0   \n",
       "2                     NaN                         0   \n",
       "3                     NaN                         0   \n",
       "4                     NaN                         0   \n",
       "\n",
       "   binary_target_cash_operations  \n",
       "0                              0  \n",
       "1                              0  \n",
       "2                              0  \n",
       "3                              0  \n",
       "4                              0  \n",
       "\n",
       "[5 rows x 670 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_canada = pd.read_csv('canada_updated.csv')\n",
    "df_canada[\"date\"] = pd.to_datetime(df_canada[\"date\"])\n",
    "df_canada.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'Ground Transportation', 'Banks',\n",
       "       'Semiconductor Equipment & Products', 'Specialty Retail',\n",
       "       'Financial Services', 'Communications Equipment',\n",
       "       'Electronic Equipment, Instruments & Components',\n",
       "       'Passenger Airlines', 'Machinery',\n",
       "       'Commercial Services & Supplies', 'Automobile Components', 'Media',\n",
       "       'Consumer Staples Distribution & Retail', 'Metals & Mining',\n",
       "       'Software', 'Health Care Providers & Services', 'Biotechnology',\n",
       "       'Hotels, Restaurants & Leisure', 'Electric Utilities',\n",
       "       'Pharmaceuticals', 'Internet Software & Services', 'Chemicals',\n",
       "       'IT Services', 'Insurance', 'Containers & Packaging',\n",
       "       'Oil, Gas & Consumable Fuels', 'Industrial Conglomerates',\n",
       "       'Trading Companies & Distributors', 'Paper & Forest Products',\n",
       "       'Energy Equipment & Services',\n",
       "       'Diversified Telecommunication Services', 'Broadline Retail',\n",
       "       'Aerospace & Defense', 'Construction & Engineering',\n",
       "       'Technology Hardware, Storage & Peripherals', 'Gas Utilities',\n",
       "       'Building Products', 'Real Estate', 'Beverages', 'Food Products',\n",
       "       'Multi-Utilities', 'Construction Materials',\n",
       "       'Wireless Telecommunication Services', 'Distributors',\n",
       "       'Electrical Equipment', 'Transportation Infrastructure',\n",
       "       'Personal Care Products', 'Health Care Equipment & Supplies',\n",
       "       'Marine Transportation', 'Undefined', 'Tobacco',\n",
       "       'Household Durables', 'Textiles, Apparel & Luxury Goods',\n",
       "       'Leisure Products', 'Semiconductors & Semiconductor Equipment',\n",
       "       'Capital Markets', 'Thrifts & Mortgage Finance',\n",
       "       'Household Products',\n",
       "       'Independent Power & Renewable Electricity Producers',\n",
       "       'Diversified Consumer Services', 'Consumer Finance',\n",
       "       'Real Estate Investment Trusts (REITs)',\n",
       "       'Real Estate Management & Development',\n",
       "       'Life Sciences Tools & Services', 'Air Freight & Logistics',\n",
       "       'Professional Services', 'Internet & Direct Marketing Retail',\n",
       "       'Diversified REITs', 'Entertainment',\n",
       "       'Interactive Media & Services', 'Retail REITs',\n",
       "       'Residential REITs', 'Office REITs', 'Industrial REITs',\n",
       "       'Specialized REITs', 'Health Care REITs'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_canada[\"industry_raw\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "industry_raw\n",
       "Metals & Mining                8554\n",
       "Oil, Gas & Consumable Fuels    7239\n",
       "Energy Equipment & Services    1770\n",
       "Media                          1424\n",
       "Capital Markets                1266\n",
       "                               ... \n",
       "Office REITs                     16\n",
       "Industrial REITs                 14\n",
       "Household Products               13\n",
       "Specialized REITs                 7\n",
       "Health Care REITs                 7\n",
       "Name: count, Length: 76, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_canada[\"industry_raw\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copie pour modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_canada.copy()\n",
    "\n",
    "df_model['date'] = pd.to_datetime(df_model['date'], errors='coerce')\n",
    "df_model.sort_values(by=['cid', 'date'], inplace=True)\n",
    "\n",
    "# Retirer les lignes où Quality_Flag est False\n",
    "df_model = df_model[df_model['QUALITY_FLAG'] == True]\n",
    "\n",
    "# (FACULTATIF) Exclure les banques\n",
    "# df_model = df_model[df_model['industry'] != 'Banks']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(\n",
    "    df,\n",
    "    include_agro=False,\n",
    "    include_rgro=False,\n",
    "    include_tcgro=False,\n",
    "    include_ratios_assets=False,\n",
    "    include_ratios_rev=False,\n",
    "    include_ratios_totcap=False,\n",
    "    mandatory_cols=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Sélectionne dynamiquement les colonnes d'un DataFrame en fonction\n",
    "    des familles de variables explicatives demandées,\n",
    "    en plaçant d'abord les colonnes obligatoires (mandatory_cols).\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1) Gérer la liste mandatory_cols (par défaut : vide ou ['cid','date'] selon besoin)\n",
    "    if mandatory_cols is None:\n",
    "        mandatory_cols = []\n",
    "    \n",
    "    # 2) Définir les \"familles\" de motifs\n",
    "    family_patterns = {\n",
    "        'agro': ['_agro_1q', '_agro_4q'],\n",
    "        'rgro': ['_rgro_1q', '_rgro_4q'],\n",
    "        'tcgro': ['_tcgro_1q', '_tcgro_4q'],\n",
    "        'ratios_assets': ['_on_assets_ratio'],\n",
    "        'ratios_rev': ['_on_rev_ratio'],\n",
    "        'ratios_totcap': ['_on_tot_cap_ratio']\n",
    "    }\n",
    "    \n",
    "    # 3) Construire la liste des motifs à inclure\n",
    "    patterns_to_keep = []\n",
    "    if include_agro:\n",
    "        patterns_to_keep += family_patterns['agro']\n",
    "    if include_rgro:\n",
    "        patterns_to_keep += family_patterns['rgro']\n",
    "    if include_tcgro:\n",
    "        patterns_to_keep += family_patterns['tcgro']\n",
    "    \n",
    "    if include_ratios_assets:\n",
    "        patterns_to_keep += family_patterns['ratios_assets']\n",
    "    if include_ratios_rev:\n",
    "        patterns_to_keep += family_patterns['ratios_rev']\n",
    "    if include_ratios_totcap:\n",
    "        patterns_to_keep += family_patterns['ratios_totcap']\n",
    "    \n",
    "    # 4) Retrouver toutes les colonnes du df qui matchent nos motifs\n",
    "    matched_cols = set()\n",
    "    for pat in patterns_to_keep:\n",
    "        for col in df.columns:\n",
    "            if pat in col:\n",
    "                matched_cols.add(col)\n",
    "    # => matched_cols est un set() de colonnes\n",
    "    \n",
    "    # 5) Conserver l'ordre original des colonnes matched, \n",
    "    #    en filtrant df.columns dans l'ordre d'origine\n",
    "    matched_cols_in_order = [c for c in df.columns if c in matched_cols]\n",
    "    \n",
    "    # 6) Construire l'ordre final :\n",
    "    #    - d'abord mandatory_cols (dans l'ordre donné),\n",
    "    #    - puis les matched_cols (dans l'ordre d'origine)\n",
    "    #    - attention aux colonnes obligatoires qui n'existent pas, \n",
    "    #      ou aux duplications\n",
    "    #    - on fait donc une intersection + un set() pour éviter \n",
    "    #      les collisions.\n",
    "    \n",
    "    # Intersection pour ne pas inclure des mandatory inexistantes\n",
    "    mandatory_cols_in_df = [c for c in mandatory_cols if c in df.columns]\n",
    "    \n",
    "    # Puis on concatène en évitant toute duplication\n",
    "    columns_to_keep_ordered = mandatory_cols_in_df + [\n",
    "        c for c in matched_cols_in_order if c not in mandatory_cols_in_df\n",
    "    ]\n",
    "    \n",
    "    # 7) Extraire le sous-DataFrame\n",
    "    df_filtered = df[columns_to_keep_ordered].copy()\n",
    "    \n",
    "    # 8) (Optionnel) trier par cid/date si elles sont présentes\n",
    "    if 'cid' in df_filtered.columns and 'date' in df_filtered.columns:\n",
    "        df_filtered.sort_values(by=['cid', 'date'], inplace=True)\n",
    "    \n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de valeurs NaN dans df_model_final : 443777\n",
      "Nombre total de valeurs NaN après suppression : 0\n",
      "(37311, 195)\n"
     ]
    }
   ],
   "source": [
    "df_model_final = select_features(\n",
    "    df_model,\n",
    "    include_agro=True,\n",
    "    include_rgro=False,\n",
    "    include_tcgro=False,\n",
    "    include_ratios_assets=True,\n",
    "    include_ratios_rev=False,\n",
    "    include_ratios_totcap=False,\n",
    "    mandatory_cols=['cid', 'date', 'binary_target_net_income']  # je garde la target\n",
    ")\n",
    "\n",
    "# Compter le nombre total de NaN dans tout le DataFrame\n",
    "total_nan = df_model_final.isna().sum().sum()\n",
    "print(f\"Nombre total de valeurs NaN dans df_model_final : {total_nan}\")\n",
    "\n",
    "# Retirer les lignes qui contiennent AU MOINS un NaN\n",
    "df_model_final.dropna(inplace=True)\n",
    "\n",
    "# Vérifier à nouveau qu’il n’y a plus de NaN\n",
    "total_nan_apres = df_model_final.isna().sum().sum()\n",
    "print(f\"Nombre total de valeurs NaN après suppression : {total_nan_apres}\")\n",
    "\n",
    "print(df_model_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DF Test pour tester la fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-12 00:00:00 2024-12-12 00:00:00\n",
      "(7552, 195)\n"
     ]
    }
   ],
   "source": [
    "# Date la plus récente du DataFrame\n",
    "max_date = df_model_final['date'].max()\n",
    "\n",
    "# Date de coupure (5 ans avant)\n",
    "cutoff_date = max_date - pd.DateOffset(years=5)\n",
    "\n",
    "# Filtrer pour ne garder que les 5 dernières années\n",
    "df_test = df_model_final[df_model_final['date'] >= cutoff_date].copy()\n",
    "\n",
    "print(df_test['date'].min(), df_test['date'].max())\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Window + AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_rolling_windows(data, date_col, target_col, train_years, val_years, test_years, buffer_months=0):\n",
    "    \"\"\"\n",
    "    Pipeline direct pour la rolling window avec AutoML et cross-validation personnalisée.\n",
    "    Ajoute les périodes dans le DataFrame final pour validation.\n",
    "    \"\"\"\n",
    "    # Conversion de la colonne date\n",
    "    data[date_col] = pd.to_datetime(data[date_col])\n",
    "    start_date = data[date_col].min()\n",
    "    end_date = data[date_col].max()\n",
    "\n",
    "    predictions_all = []  # Liste pour stocker toutes les prédictions\n",
    "\n",
    "    while start_date + relativedelta(years=train_years + val_years + test_years) <= end_date:\n",
    "        # Définir les périodes\n",
    "        train_end = start_date + relativedelta(years=train_years) - pd.Timedelta(days=1)\n",
    "        tampon_1_end = train_end + relativedelta(months=buffer_months)\n",
    "        val_start = tampon_1_end + pd.Timedelta(days=1)\n",
    "        val_end = val_start + relativedelta(years=val_years) - pd.Timedelta(days=1)\n",
    "        tampon_2_end = val_end + relativedelta(months=buffer_months)\n",
    "        test_start = tampon_2_end + pd.Timedelta(days=1)\n",
    "        test_end = test_start + relativedelta(years=test_years) - pd.Timedelta(days=1)\n",
    "\n",
    "        # Filtrer les données\n",
    "        train_data = data.loc[(data[date_col] >= start_date) & (data[date_col] <= train_end)]\n",
    "        val_data = data.loc[(data[date_col] >= val_start) & (data[date_col] <= val_end)]\n",
    "        test_data = data.loc[(data[date_col] >= test_start) & (data[date_col] <= test_end)]\n",
    "\n",
    "        if len(train_data) == 0 or len(val_data) == 0 or len(test_data) == 0:\n",
    "            print(f\"Fenêtre {start_date.year}-{test_end.year} : données insuffisantes, sautée.\")\n",
    "            start_date += relativedelta(years=1)\n",
    "            continue\n",
    "\n",
    "        # Configurer et entraîner AutoML\n",
    "        print(f\"Fenêtre {start_date.year}-{test_end.year} : entraînement de AutoML...\")\n",
    "        automl = AutoML(results_path=f\"AutoML_{start_date.year}-{test_end.year}\", mode=\"Perform\", algorithms=[\"Xgboost\"], eval_metric=\"auc\")\n",
    "        custom_cv = [(train_data.index, val_data.index)]\n",
    "        automl.fit(\n",
    "            train_data.drop(columns=[target_col, date_col, 'cid']),\n",
    "            train_data[target_col], cv=custom_cv\n",
    "        )\n",
    "\n",
    "        # Prédire sur le test set\n",
    "        test_preds = test_data[[date_col, target_col]].copy()\n",
    "        #test_preds[\"predicted\"] = automl.predict_proba(test_data.drop(columns=[target_col, date_col, 'cid']))\n",
    "        proba = automl.predict_proba(test_data.drop(columns=[target_col, date_col, 'cid']))\n",
    "        test_preds[\"prob_down\"] = proba[:, 0]  # Probabilité pour la classe 0 (diminution des bénéfices)\n",
    "        test_preds[\"prob_up\"] = proba[:, 1]  # Probabilité pour la classe 1 (augmentation des bénéfices)\n",
    "        test_preds[\"net_prob_score\"] = test_preds[\"prob_up\"] - test_preds[\"prob_down\"]\n",
    "        test_preds[\"window\"] = f\"{start_date.year}-{test_end.year}\"\n",
    "        test_preds[\"cid\"] = test_data[\"cid\"].values\n",
    "\n",
    "        # Ajouter les périodes pour validation\n",
    "        #test_preds[\"train_start\"] = start_date\n",
    "        #test_preds[\"train_end\"] = train_end\n",
    "        #test_preds[\"tampon_1\"] = tampon_1_end\n",
    "        #test_preds[\"val_start\"] = val_start\n",
    "        #test_preds[\"val_end\"] = val_end\n",
    "        #test_preds[\"tampon_2\"] = tampon_2_end\n",
    "        #test_preds[\"test_start\"] = test_start\n",
    "        #test_preds[\"test_end\"] = test_end\n",
    "\n",
    "        # Sauvegarder les prédictions\n",
    "        predictions_all.append(test_preds)\n",
    "\n",
    "        # Avancer la fenêtre\n",
    "        start_date += relativedelta(years=1)\n",
    "\n",
    "    predictions_df = pd.concat(predictions_all, ignore_index=True)\n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appel de fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fenêtre 2019-2024 : entraînement de AutoML...\n",
      "AutoML directory: AutoML_2019-2024\n",
      "The task is binary_classification with evaluation metric auc\n",
      "AutoML will use algorithms: ['Xgboost']\n",
      "AutoML will ensemble available models\n",
      "AutoML steps: ['simple_algorithms', 'default_algorithms', 'not_so_random', 'golden_features', 'insert_random_feature', 'features_selection', 'hill_climbing_1', 'hill_climbing_2', 'ensemble']\n",
      "Skip simple_algorithms because no parameters were generated.\n",
      "* Step default_algorithms will try to check up to 1 model\n",
      "1_Default_Xgboost auc 0.866391 trained in 18.24 seconds (1-sample predict time 0.0343 seconds)\n",
      "* Step not_so_random will try to check up to 4 models\n",
      "2_Xgboost auc 0.860464 trained in 13.96 seconds (1-sample predict time 0.0348 seconds)\n",
      "3_Xgboost auc 0.86656 trained in 14.81 seconds (1-sample predict time 0.0341 seconds)\n",
      "4_Xgboost auc 0.844248 trained in 15.64 seconds (1-sample predict time 0.0353 seconds)\n",
      "5_Xgboost auc 0.795209 trained in 8.74 seconds (1-sample predict time 0.0362 seconds)\n",
      "* Step golden_features will try to check up to 3 models\n",
      "None 19\n",
      "Add Golden Feature: E_G_norm_net_inc_agro_1q_diff_E_G_net_income_inc_on_assets_ratio\n",
      "Add Golden Feature: E_G_norm_net_inc_on_assets_ratio_sum_E_G_retained_earnings_agro_4q\n",
      "Add Golden Feature: E_G_retained_earnings_agro_1q_diff_E_G_common_equity_agro_4q\n",
      "Add Golden Feature: E_G_ebit_on_assets_ratio_sum_E_G_net_income_inc_agro_4q\n",
      "Add Golden Feature: E_G_cash_equi_on_assets_ratio_sum_E_G_common_equity_agro_4q\n",
      "Add Golden Feature: E_G_depre_amor_on_assets_ratio_sum_E_G_common_equity_agro_4q\n",
      "Add Golden Feature: E_G_total_debt_on_assets_ratio_ratio_E_G_common_equity_agro_4q\n",
      "Add Golden Feature: E_G_total_liabilities_on_assets_ratio_multiply_E_G_net_income_inc_on_assets_ratio\n",
      "Add Golden Feature: E_G_net_income_inc_on_assets_ratio_diff_E_G_tax_expense_on_assets_ratio\n",
      "Add Golden Feature: E_G_total_div_on_assets_ratio_ratio_E_G_net_income_inc_on_assets_ratio\n",
      "Add Golden Feature: E_G_ebitda_on_assets_ratio_ratio_E_G_net_income_inc_on_assets_ratio\n",
      "Add Golden Feature: E_G_cash_investing_on_assets_ratio_sum_E_G_total_capital_agro_4q\n",
      "Add Golden Feature: E_G_net_income_inc_agro_4q_sum_E_G_current_liabilities_agro_1q\n",
      "Add Golden Feature: E_G_common_equity_agro_4q_sum_E_G_norm_net_inc_agro_1q\n",
      "Add Golden Feature: E_G_gross_property_agro_4q_sum_E_G_common_equity_agro_4q\n",
      "Add Golden Feature: E_G_net_income_inc_on_assets_ratio_sum_E_G_ebit_agro_4q\n",
      "Add Golden Feature: E_G_net_income_inc_on_assets_ratio_sum_E_G_operating_income_agro_4q\n",
      "Add Golden Feature: E_G_common_equity_agro_4q_sum_E_G_other_expenses_agro_1q\n",
      "Add Golden Feature: E_G_tangible_book_agro_1q_diff_E_G_common_equity_agro_4q\n",
      "Created 19 Golden Features in 12.67 seconds.\n",
      "3_Xgboost_GoldenFeatures auc 0.86779 trained in 28.43 seconds (1-sample predict time 0.0605 seconds)\n",
      "1_Default_Xgboost_GoldenFeatures auc 0.863885 trained in 17.97 seconds (1-sample predict time 0.0585 seconds)\n",
      "2_Xgboost_GoldenFeatures auc 0.862694 trained in 15.58 seconds (1-sample predict time 0.0594 seconds)\n",
      "* Step insert_random_feature will try to check up to 1 model\n",
      "3_Xgboost_GoldenFeatures_RandomFeature auc 0.869672 trained in 16.82 seconds (1-sample predict time 0.0561 seconds)\n",
      "Drop features ['E_G_net_income_inc_agro_4q', 'E_G_total_capital_on_assets_ratio', 'E_G_cost_borrowing_on_assets_ratio', 'E_G_total_debt_on_assets_ratio', 'E_G_def_tax_liability_agro_4q', 'E_G_interest_expense_agro_4q', 'E_G_total_liabilities_on_assets_ratio', 'E_G_payables_agro_4q', 'E_G_asset_writedown_on_assets_ratio', 'E_G_compre_income_agro_1q', 'E_G_current_assets_on_assets_ratio', 'E_G_receivables_on_assets_ratio', 'E_G_cash_financing_agro_4q', 'E_G_int_div_income_agro_4q', 'E_G_operating_income_on_assets_ratio', 'E_G_common_equity_agro_1q', 'E_G_restructuring_charges_on_assets_ratio', 'E_G_total_div_on_assets_ratio', 'E_G_cash_equi_agro_1q', 'E_G_ebitda_agro_1q', 'E_G_repurchase_stock_agro_1q', 'E_G_capex_on_assets_ratio', 'E_G_r_d_agro_4q', 'E_G_cost_borrowing_agro_1q', 'random_feature', 'E_G_ammor_intangibles_on_assets_ratio', 'E_G_total_revenues_on_assets_ratio', 'E_G_restructuring_charges_agro_1q', 'E_G_ebt_x_unusual_agro_1q', 'E_G_net_debt_on_assets_ratio', 'E_G_divestitures_agro_1q', 'E_G_sga_agro_1q', 'E_G_total_expenses_agro_1q', 'E_G_core_banks_agro_4q', 'E_G_inventory_agro_1q', 'E_G_core_banks_on_assets_ratio', 'E_G_minority_int_on_assets_ratio', 'E_G_r_d_agro_1q', 'E_G_depre_amor_on_assets_ratio_sum_E_G_common_equity_agro_4q', 'E_G_core_banks_agro_1q', 'E_G_total_expenses_agro_4q', 'E_G_pref_equity_agro_1q', 'E_G_provision_loan_agro_4q', 'E_G_total_assets_agro_1q', 'E_G_ebt_x_unusual_on_assets_ratio', 'E_G_pref_div_agro_4q', 'E_G_provision_loan_agro_1q', 'E_G_tax_expense_on_assets_ratio', 'E_G_pref_equity_agro_4q', 'E_G_total_intangibles_agro_1q', 'E_G_provision_loan_on_assets_ratio', 'E_G_net_inc_gro_five_agro_4q', 'E_G_pref_div_on_assets_ratio', 'E_G_total_debt_agro_4q', 'E_G_total_debt_on_assets_ratio_ratio_E_G_common_equity_agro_4q', 'E_G_common_equity_agro_4q_sum_E_G_norm_net_inc_agro_1q', 'E_G_quick_ratio_agro_4q', 'E_G_operating_income_agro_4q', 'E_G_net_inc_gro_five_on_assets_ratio', 'E_G_divestitures_on_assets_ratio', 'E_G_quick_ratio_agro_1q', 'E_G_inventory_agro_4q', 'E_G_common_equity_agro_4q_sum_E_G_other_expenses_agro_1q', 'E_G_unusual_items_agro_1q', 'E_G_def_tax_liability_agro_1q', 'E_G_ops_expenses_agro_1q', 'E_G_working_cap_agro_4q', 'E_G_minority_int_agro_4q', 'E_G_cash_acquisitions_agro_1q', 'E_G_tangible_book_agro_1q', 'E_G_net_inc_gro_five_agro_1q', 'E_G_cogs_agro_4q', 'E_G_cogs_agro_1q', 'E_G_ebit_on_assets_ratio', 'E_G_ops_expenses_on_assets_ratio', 'E_G_pref_div_agro_1q', 'E_G_cash_acquisitions_agro_4q', 'E_G_rev_gro_five_agro_1q', 'E_G_ebit_agro_4q', 'E_G_ebit_agro_1q', 'E_G_current_assets_agro_1q', 'E_G_ebit_on_assets_ratio_sum_E_G_net_income_inc_agro_4q', 'E_G_net_property_agro_1q', 'E_G_gross_profit_agro_4q', 'E_G_current_assets_agro_4q', 'E_G_cash_operations_agro_1q', 'E_G_gross_property_agro_1q', 'E_G_common_equity_agro_4q', 'E_G_total_debt_agro_1q', 'E_G_cash_investing_agro_1q', 'E_G_cash_financing_on_assets_ratio', 'E_G_working_cap_agro_1q', 'E_G_other_expenses_agro_1q', 'E_G_net_income_inc_agro_4q_sum_E_G_current_liabilities_agro_1q', 'E_G_net_income_inc_on_assets_ratio_sum_E_G_ebit_agro_4q', 'E_G_assets_gro_five_agro_1q', 'E_G_retained_earnings_agro_1q', 'E_G_norm_net_inc_on_assets_ratio_sum_E_G_retained_earnings_agro_4q', 'E_G_total_capital_agro_1q', 'E_G_operating_income_agro_1q', 'E_G_net_income_inc_on_assets_ratio_sum_E_G_operating_income_agro_4q', 'E_G_cash_equi_on_assets_ratio_sum_E_G_common_equity_agro_4q', 'E_G_total_capital_agro_4q', 'E_G_int_div_income_on_assets_ratio', 'E_G_payables_agro_1q', 'E_G_retained_earnings_agro_4q', 'E_G_cash_investing_on_assets_ratio_sum_E_G_total_capital_agro_4q', 'E_G_gross_property_agro_4q_sum_E_G_common_equity_agro_4q', 'E_G_retained_earnings_agro_1q_diff_E_G_common_equity_agro_4q', 'E_G_tangible_book_agro_1q_diff_E_G_common_equity_agro_4q']\n",
      "* Step features_selection will try to check up to 1 model\n",
      "3_Xgboost_GoldenFeatures_SelectedFeatures auc 0.888361 trained in 12.16 seconds (1-sample predict time 0.0329 seconds)\n",
      "* Step hill_climbing_1 will try to check up to 4 models\n",
      "6_Xgboost_GoldenFeatures_SelectedFeatures auc 0.887913 trained in 13.0 seconds (1-sample predict time 0.0332 seconds)\n",
      "7_Xgboost_GoldenFeatures_SelectedFeatures auc 0.884887 trained in 11.16 seconds (1-sample predict time 0.0329 seconds)\n",
      "8_Xgboost_GoldenFeatures auc 0.869772 trained in 17.16 seconds (1-sample predict time 0.0549 seconds)\n",
      "9_Xgboost_GoldenFeatures auc 0.868422 trained in 15.69 seconds (1-sample predict time 0.0553 seconds)\n",
      "* Step hill_climbing_2 will try to check up to 2 models\n",
      "10_Xgboost_GoldenFeatures_SelectedFeatures auc 0.888007 trained in 10.73 seconds (1-sample predict time 0.0327 seconds)\n",
      "11_Xgboost_GoldenFeatures_SelectedFeatures auc 0.888238 trained in 14.66 seconds (1-sample predict time 0.0327 seconds)\n",
      "* Step ensemble will try to check up to 1 model\n",
      "Ensemble auc 0.892118 trained in 1.13 seconds (1-sample predict time 0.1298 seconds)\n",
      "AutoML fit time: 261.68 seconds\n",
      "AutoML best model: Ensemble\n",
      "Fenêtre 2020-2025 : entraînement de AutoML...\n",
      "AutoML directory: AutoML_2020-2025\n",
      "The task is binary_classification with evaluation metric auc\n",
      "AutoML will use algorithms: ['Xgboost']\n",
      "AutoML will ensemble available models\n",
      "AutoML steps: ['simple_algorithms', 'default_algorithms', 'not_so_random', 'golden_features', 'insert_random_feature', 'features_selection', 'hill_climbing_1', 'hill_climbing_2', 'ensemble']\n",
      "Skip simple_algorithms because no parameters were generated.\n",
      "* Step default_algorithms will try to check up to 1 model\n",
      "1_Default_Xgboost auc 0.889693 trained in 14.42 seconds (1-sample predict time 0.034 seconds)\n",
      "* Step not_so_random will try to check up to 4 models\n",
      "2_Xgboost auc 0.88546 trained in 15.54 seconds (1-sample predict time 0.0341 seconds)\n",
      "3_Xgboost auc 0.889247 trained in 13.71 seconds (1-sample predict time 0.0336 seconds)\n",
      "4_Xgboost auc 0.86638 trained in 11.36 seconds (1-sample predict time 0.0342 seconds)\n",
      "5_Xgboost auc 0.821975 trained in 8.64 seconds (1-sample predict time 0.0337 seconds)\n",
      "* Step golden_features will try to check up to 3 models\n",
      "None 19\n",
      "Add Golden Feature: E_G_ebt_x_unusual_agro_1q_diff_E_G_retained_earnings_agro_4q\n",
      "Add Golden Feature: E_G_common_equity_agro_4q_diff_E_G_issuance_stock_on_assets_ratio\n",
      "Add Golden Feature: E_G_retained_earnings_agro_4q_sum_E_G_other_expenses_agro_4q\n",
      "Add Golden Feature: E_G_common_equity_agro_4q_diff_E_G_shares_out_agro_4q\n",
      "Add Golden Feature: E_G_cash_equi_on_assets_ratio_multiply_E_G_common_equity_agro_4q\n",
      "Add Golden Feature: E_G_unusual_items_on_assets_ratio_sum_E_G_net_income_inc_on_assets_ratio\n",
      "Add Golden Feature: E_G_common_equity_agro_4q_sum_E_G_assets_gro_five_agro_4q\n",
      "Add Golden Feature: E_G_retained_earnings_agro_4q_sum_E_G_cash_equi_agro_1q\n",
      "Add Golden Feature: E_G_depre_amor_on_assets_ratio_ratio_E_G_common_equity_agro_4q\n",
      "Add Golden Feature: E_G_common_equity_agro_4q_ratio_E_G_interest_expense_on_assets_ratio\n",
      "Add Golden Feature: E_G_working_cap_agro_1q_diff_E_G_common_equity_agro_4q\n",
      "Add Golden Feature: E_G_common_equity_agro_4q_diff_E_G_restructuring_charges_agro_4q\n",
      "Add Golden Feature: E_G_total_intangibles_agro_4q_sum_E_G_common_equity_agro_4q\n",
      "Add Golden Feature: E_G_ops_expenses_on_assets_ratio_ratio_E_G_retained_earnings_agro_4q\n",
      "Add Golden Feature: E_G_retained_earnings_agro_4q_diff_E_G_compre_income_on_assets_ratio\n",
      "Add Golden Feature: E_G_operating_income_agro_1q_diff_E_G_retained_earnings_agro_4q\n",
      "Add Golden Feature: E_G_common_equity_agro_4q_ratio_E_G_cost_borrowing_on_assets_ratio\n",
      "Add Golden Feature: E_G_ebit_agro_1q_diff_E_G_retained_earnings_agro_4q\n",
      "Add Golden Feature: E_G_tangible_book_agro_4q_diff_E_G_cash_acquisitions_on_assets_ratio\n",
      "Created 19 Golden Features in 12.93 seconds.\n",
      "1_Default_Xgboost_GoldenFeatures auc 0.88556 trained in 29.46 seconds (1-sample predict time 0.0548 seconds)\n",
      "3_Xgboost_GoldenFeatures auc 0.890096 trained in 19.76 seconds (1-sample predict time 0.0555 seconds)\n",
      "2_Xgboost_GoldenFeatures auc 0.883005 trained in 16.71 seconds (1-sample predict time 0.0555 seconds)\n",
      "* Step insert_random_feature will try to check up to 1 model\n",
      "3_Xgboost_GoldenFeatures_RandomFeature auc 0.887082 trained in 19.64 seconds (1-sample predict time 0.0563 seconds)\n",
      "Drop features ['E_G_inventory_agro_4q', 'E_G_minority_int_agro_4q', 'E_G_net_debt_on_assets_ratio', 'E_G_effective_tax_rate_agro_1q', 'E_G_receivables_agro_1q', 'E_G_receivables_on_assets_ratio', 'E_G_tangible_book_agro_1q', 'E_G_working_cap_on_assets_ratio', 'E_G_assets_gro_five_agro_4q', 'E_G_gross_property_on_assets_ratio', 'E_G_shares_out_agro_1q', 'E_G_cash_financing_agro_1q', 'E_G_provision_loan_agro_1q', 'E_G_compre_income_agro_4q', 'E_G_total_liabilities_on_assets_ratio', 'E_G_operating_income_agro_1q_diff_E_G_retained_earnings_agro_4q', 'E_G_rev_gro_five_agro_1q', 'E_G_total_assets_agro_1q', 'E_G_total_revenues_on_assets_ratio', 'E_G_gross_profit_on_assets_ratio', 'E_G_operating_income_agro_4q', 'E_G_interest_expense_on_assets_ratio', 'E_G_net_inc_gro_five_agro_1q', 'E_G_net_income_inc_agro_1q', 'E_G_pref_equity_agro_4q', 'E_G_total_debt_agro_4q', 'E_G_common_equity_on_assets_ratio', 'E_G_shares_out_on_assets_ratio', 'E_G_ebt_x_unusual_agro_4q', 'E_G_cost_borrowing_agro_1q', 'E_G_cash_acquisitions_agro_4q', 'E_G_pref_equity_on_assets_ratio', 'E_G_retained_earnings_agro_1q', 'E_G_net_property_agro_4q', 'E_G_cash_equi_on_assets_ratio_multiply_E_G_common_equity_agro_4q', 'E_G_ops_expenses_on_assets_ratio', 'E_G_common_equity_agro_4q_sum_E_G_assets_gro_five_agro_4q', 'E_G_interest_expense_agro_4q', 'E_G_core_banks_agro_4q', 'E_G_sga_agro_1q', 'E_G_norm_net_inc_on_assets_ratio', 'E_G_def_tax_assets_agro_1q', 'E_G_pref_div_agro_1q', 'E_G_capex_agro_4q', 'E_G_issuance_stock_agro_4q', 'E_G_divestitures_agro_1q', 'E_G_core_banks_agro_1q', 'E_G_provision_loan_on_assets_ratio', 'E_G_tax_expense_on_assets_ratio', 'E_G_total_expenses_agro_4q', 'E_G_total_liabilities_agro_1q', 'E_G_core_banks_on_assets_ratio', 'E_G_pref_equity_agro_1q', 'E_G_pref_div_on_assets_ratio', 'E_G_divestitures_on_assets_ratio', 'E_G_r_d_agro_4q', 'E_G_net_inc_gro_five_agro_4q', 'E_G_provision_loan_agro_4q', 'E_G_r_d_agro_1q', 'E_G_total_expenses_on_assets_ratio', 'E_G_total_expenses_agro_1q', 'E_G_ebitda_agro_4q', 'E_G_ebit_agro_4q', 'E_G_r_d_on_assets_ratio', 'E_G_ops_expenses_agro_4q', 'E_G_asset_writedown_agro_1q', 'E_G_pref_div_agro_4q', 'E_G_total_div_agro_1q', 'E_G_capex_agro_1q', 'E_G_unusual_items_on_assets_ratio', 'E_G_issuance_stock_on_assets_ratio', 'E_G_restructuring_charges_agro_1q', 'E_G_ops_expenses_agro_1q', 'E_G_interest_expense_agro_1q', 'E_G_int_div_income_agro_1q', 'E_G_gross_property_agro_1q', 'random_feature', 'E_G_total_assets_agro_4q', 'E_G_issuance_stock_agro_1q', 'E_G_ebt_x_unusual_agro_1q', 'E_G_def_tax_liability_agro_1q', 'E_G_norm_net_inc_agro_4q', 'E_G_cash_acquisitions_agro_1q', 'E_G_common_equity_agro_4q', 'E_G_total_revenues_agro_4q', 'E_G_cash_investing_agro_1q', 'E_G_cash_equi_agro_1q', 'E_G_asset_writedown_on_assets_ratio', 'E_G_total_capital_agro_1q', 'E_G_ebitda_agro_1q', 'E_G_ops_expenses_on_assets_ratio_ratio_E_G_retained_earnings_agro_4q', 'E_G_shares_out_agro_4q', 'E_G_minority_int_agro_1q', 'E_G_total_capital_agro_4q', 'E_G_tangible_book_agro_4q', 'E_G_restructuring_charges_agro_4q', 'E_G_retained_earnings_agro_4q_diff_E_G_compre_income_on_assets_ratio', 'E_G_common_equity_agro_4q_diff_E_G_shares_out_agro_4q', 'E_G_net_property_agro_1q', 'E_G_net_inc_gro_five_on_assets_ratio', 'E_G_cogs_agro_1q', 'E_G_working_cap_agro_4q', 'E_G_common_equity_agro_4q_diff_E_G_restructuring_charges_agro_4q', 'E_G_tangible_book_agro_4q_diff_E_G_cash_acquisitions_on_assets_ratio', 'E_G_total_revenues_agro_1q', 'E_G_ebit_on_assets_ratio', 'E_G_repurchase_stock_agro_1q', 'E_G_cash_financing_agro_4q', 'E_G_current_liabilities_agro_1q', 'E_G_common_equity_agro_4q_ratio_E_G_interest_expense_on_assets_ratio', 'E_G_operating_income_agro_1q', 'E_G_retained_earnings_agro_4q', 'E_G_retained_earnings_agro_4q_sum_E_G_cash_equi_agro_1q', 'E_G_current_assets_agro_1q', 'E_G_total_intangibles_agro_4q_sum_E_G_common_equity_agro_4q', 'E_G_ebt_x_unusual_on_assets_ratio', 'E_G_common_equity_agro_4q_diff_E_G_issuance_stock_on_assets_ratio', 'E_G_retained_earnings_agro_4q_sum_E_G_other_expenses_agro_4q', 'E_G_depre_amor_on_assets_ratio_ratio_E_G_common_equity_agro_4q', 'E_G_ebit_agro_1q_diff_E_G_retained_earnings_agro_4q', 'E_G_working_cap_agro_1q_diff_E_G_common_equity_agro_4q']\n",
      "* Step features_selection will try to check up to 1 model\n",
      "3_Xgboost_GoldenFeatures_SelectedFeatures auc 0.899827 trained in 12.57 seconds (1-sample predict time 0.0301 seconds)\n",
      "* Step hill_climbing_1 will try to check up to 4 models\n",
      "6_Xgboost_GoldenFeatures_SelectedFeatures auc 0.898523 trained in 13.08 seconds (1-sample predict time 0.0303 seconds)\n",
      "7_Xgboost_GoldenFeatures_SelectedFeatures auc 0.897695 trained in 10.63 seconds (1-sample predict time 0.0307 seconds)\n",
      "8_Xgboost_GoldenFeatures auc 0.884158 trained in 19.72 seconds (1-sample predict time 0.0552 seconds)\n",
      "9_Xgboost_GoldenFeatures auc 0.886671 trained in 14.62 seconds (1-sample predict time 0.0551 seconds)\n",
      "* Step hill_climbing_2 will try to check up to 2 models\n",
      "10_Xgboost_GoldenFeatures_SelectedFeatures auc 0.90295 trained in 11.62 seconds (1-sample predict time 0.0302 seconds)\n",
      "11_Xgboost_GoldenFeatures_SelectedFeatures auc 0.9023 trained in 13.33 seconds (1-sample predict time 0.0304 seconds)\n",
      "* Step ensemble will try to check up to 1 model\n",
      "Ensemble auc 0.905888 trained in 1.13 seconds (1-sample predict time 0.1558 seconds)\n",
      "AutoML fit time: 261.75 seconds\n",
      "AutoML best model: Ensemble\n"
     ]
    }
   ],
   "source": [
    "predictions_df = pipeline_rolling_windows(\n",
    "    data=df_test, \n",
    "    date_col=\"date\", \n",
    "    target_col=\"binary_target_net_income\", \n",
    "    train_years=2, \n",
    "    val_years=1, \n",
    "    test_years=1, \n",
    "    buffer_months=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.to_csv(\"df_test_2025.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped AUCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_auc_by_groups(\n",
    "    predictions_df: pd.DataFrame, \n",
    "    df_canada: pd.DataFrame,\n",
    "    group_cols: list,\n",
    "    cid_col: str = \"cid\",\n",
    "    date_col: str = \"date\",\n",
    "    target_col: str = \"binary_target_net_income\",\n",
    "    prob_col: str = \"prob_up\",\n",
    "    out_csv: str = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Calcule l'AUC par groupes (selon group_cols) et le nombre de lignes dans chaque groupe.\n",
    "    \n",
    "    Arguments :\n",
    "    -----------\n",
    "    - predictions_df : DataFrame contenant au moins [cid_col, date_col, target_col, prob_col]\n",
    "    - df_canada : DataFrame contenant au moins [cid_col, date_col, 'return_1q', 'industry_raw', etc.]\n",
    "    - group_cols : liste des colonnes sur lesquelles faire le groupby (ex: [\"year\", \"industry_raw\"])\n",
    "    - cid_col : nom de la colonne identifiant l'entreprise (défaut: \"cid\")\n",
    "    - date_col : nom de la colonne date (défaut: \"date\")\n",
    "    - target_col : nom de la colonne cible binaire (ex: 0/1) (défaut: \"binary_target_net_income\")\n",
    "    - prob_col : nom de la colonne contenant la probabilité pour la classe positive (défaut: \"prob_up\")\n",
    "    - out_csv : chemin vers le fichier CSV de sortie (défaut: None, ne pas exporter)\n",
    "\n",
    "    Retour :\n",
    "    --------\n",
    "    - final_df : DataFrame avec AUC et Count pour chaque groupe\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Fusionner les dataframes (left merge pour conserver toutes les prédictions)\n",
    "    df_merged = predictions_df.merge(\n",
    "        df_canada[[cid_col, date_col, 'return_1q', 'industry_raw']],  # adapt si tu veux plus/moins de colonnes\n",
    "        on=[cid_col, date_col],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # 2) Si \"year\" est dans group_cols et pas encore créée, on l'extrait du date_col\n",
    "    if \"year\" in group_cols and \"year\" not in df_merged.columns:\n",
    "        df_merged[\"year\"] = pd.to_datetime(df_merged[date_col]).dt.year\n",
    "\n",
    "    # -- Fonction interne pour calculer l’AUC sur un sous-groupe\n",
    "    def group_auc(sub_df):\n",
    "        unique_targets = sub_df[target_col].unique()\n",
    "        # Gérer le cas où il n’y a qu’une seule classe (roc_auc_score plantait sinon)\n",
    "        if len(unique_targets) < 2:\n",
    "            return float(\"nan\")\n",
    "        return roc_auc_score(sub_df[target_col], sub_df[prob_col])\n",
    "\n",
    "    # 3) Calculer l’AUC par groupe\n",
    "    grouped_aucs = (\n",
    "        df_merged\n",
    "        .groupby(group_cols)\n",
    "        .apply(group_auc)\n",
    "        .reset_index(name=\"AUC\")\n",
    "    )\n",
    "\n",
    "    # 4) Calculer le nombre d’observations par groupe (ou nombre de cid uniques, selon besoin)\n",
    "    #    Ici : on compte simplement le nombre de lignes\n",
    "    grouped_counts = (\n",
    "        df_merged\n",
    "        .groupby(group_cols)[target_col]  # ou [cid_col] si tu veux le nombre d'entreprises distinctes\n",
    "        .count()\n",
    "        .reset_index(name=\"Count\")\n",
    "    )\n",
    "\n",
    "    # 5) Fusionner AUC et Count\n",
    "    final_df = pd.merge(grouped_aucs, grouped_counts, on=group_cols, how=\"left\")\n",
    "\n",
    "    # Optionnel : trier le résultat\n",
    "    final_df = final_df.sort_values(group_cols)\n",
    "\n",
    "    # Optionnel : exporter en CSV\n",
    "    if out_csv is not None:\n",
    "        final_df.to_csv(out_csv, index=False)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ns/nlrkcsjd7j70p9jvf2jl5t4c0000gn/T/ipykernel_1219/2025299659.py:53: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>industry_raw</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>Aerospace &amp; Defense</td>\n",
       "      <td>0.987500</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>Air Freight &amp; Logistics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023</td>\n",
       "      <td>Automobile Components</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023</td>\n",
       "      <td>Banks</td>\n",
       "      <td>0.655797</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>2024</td>\n",
       "      <td>Specialty Retail</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>2024</td>\n",
       "      <td>Textiles, Apparel &amp; Luxury Goods</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>2024</td>\n",
       "      <td>Trading Companies &amp; Distributors</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>2024</td>\n",
       "      <td>Transportation Infrastructure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>2024</td>\n",
       "      <td>Wireless Telecommunication Services</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     year                         industry_raw       AUC  Count\n",
       "0    2023                  Aerospace & Defense  0.987500     18\n",
       "1    2023              Air Freight & Logistics       NaN      4\n",
       "2    2023                Automobile Components  0.600000     19\n",
       "3    2023                                Banks  0.655797     35\n",
       "4    2023                            Beverages  1.000000      3\n",
       "..    ...                                  ...       ...    ...\n",
       "113  2024                     Specialty Retail       NaN     26\n",
       "114  2024     Textiles, Apparel & Luxury Goods       NaN      8\n",
       "115  2024     Trading Companies & Distributors       NaN     31\n",
       "116  2024        Transportation Infrastructure       NaN      4\n",
       "117  2024  Wireless Telecommunication Services       NaN      5\n",
       "\n",
       "[118 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_auc_by_groups(\n",
    "    predictions_df=predictions_df,\n",
    "    df_canada=df_canada,\n",
    "    group_cols=[\"year\", \"industry_raw\"],\n",
    "    out_csv=\"auc_par_annee_et_industrie.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ns/nlrkcsjd7j70p9jvf2jl5t4c0000gn/T/ipykernel_1219/2025299659.py:53: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>industry_raw</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aerospace &amp; Defense</td>\n",
       "      <td>0.901786</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Air Freight &amp; Logistics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Automobile Components</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Banks</td>\n",
       "      <td>0.816038</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Beverages</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Biotechnology</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Broadline Retail</td>\n",
       "      <td>0.236364</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Capital Markets</td>\n",
       "      <td>0.676721</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Chemicals</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Commercial Services &amp; Supplies</td>\n",
       "      <td>0.631452</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Communications Equipment</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Construction &amp; Engineering</td>\n",
       "      <td>0.727513</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Consumer Finance</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Consumer Staples Distribution &amp; Retail</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Containers &amp; Packaging</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Distributors</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Diversified Consumer Services</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Diversified REITs</td>\n",
       "      <td>0.606593</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Diversified Telecommunication Services</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Electric Utilities</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Electrical Equipment</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Electronic Equipment, Instruments &amp; Components</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Energy Equipment &amp; Services</td>\n",
       "      <td>0.682581</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Entertainment</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Financial Services</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Food Products</td>\n",
       "      <td>0.720588</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Gas Utilities</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Ground Transportation</td>\n",
       "      <td>0.376344</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Health Care Equipment &amp; Supplies</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Health Care Providers &amp; Services</td>\n",
       "      <td>0.712610</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Health Care REITs</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Hotels, Restaurants &amp; Leisure</td>\n",
       "      <td>0.802273</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>IT Services</td>\n",
       "      <td>0.765385</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Independent Power &amp; Renewable Electricity Prod...</td>\n",
       "      <td>0.626488</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Industrial REITs</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Insurance</td>\n",
       "      <td>0.794534</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Leisure Products</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Machinery</td>\n",
       "      <td>0.860947</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Marine Transportation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Media</td>\n",
       "      <td>0.706731</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Metals &amp; Mining</td>\n",
       "      <td>0.669971</td>\n",
       "      <td>413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Multi-Utilities</td>\n",
       "      <td>0.336134</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Office REITs</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Oil, Gas &amp; Consumable Fuels</td>\n",
       "      <td>0.593248</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Paper &amp; Forest Products</td>\n",
       "      <td>0.585000</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Passenger Airlines</td>\n",
       "      <td>0.657407</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Personal Care Products</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Pharmaceuticals</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Professional Services</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Real Estate Management &amp; Development</td>\n",
       "      <td>0.753458</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Residential REITs</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Retail REITs</td>\n",
       "      <td>0.594907</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Software</td>\n",
       "      <td>0.719313</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Specialized REITs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Specialty Retail</td>\n",
       "      <td>0.826241</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Textiles, Apparel &amp; Luxury Goods</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Thrifts &amp; Mortgage Finance</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Trading Companies &amp; Distributors</td>\n",
       "      <td>0.485380</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Transportation Infrastructure</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Wireless Telecommunication Services</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         industry_raw       AUC  Count\n",
       "0                                 Aerospace & Defense  0.901786     36\n",
       "1                             Air Freight & Logistics       NaN      8\n",
       "2                               Automobile Components  0.590909     37\n",
       "3                                               Banks  0.816038     65\n",
       "4                                           Beverages  0.700000      7\n",
       "5                                       Biotechnology  0.166667     10\n",
       "6                                    Broadline Retail  0.236364     16\n",
       "7                                     Capital Markets  0.676721     86\n",
       "8                                           Chemicals  0.687500     55\n",
       "9                      Commercial Services & Supplies  0.631452     82\n",
       "10                           Communications Equipment  0.606061     14\n",
       "11                         Construction & Engineering  0.727513     41\n",
       "12                                   Consumer Finance  0.714286     16\n",
       "13             Consumer Staples Distribution & Retail  0.765957     51\n",
       "14                             Containers & Packaging  0.468750     42\n",
       "15                                       Distributors       NaN      1\n",
       "16                      Diversified Consumer Services  0.300000     11\n",
       "17                                  Diversified REITs  0.606593     48\n",
       "18             Diversified Telecommunication Services  0.480000     27\n",
       "19                                 Electric Utilities  0.266667     23\n",
       "20                               Electrical Equipment  0.500000     17\n",
       "21     Electronic Equipment, Instruments & Components  0.750000     18\n",
       "22                        Energy Equipment & Services  0.682581    131\n",
       "23                                      Entertainment  0.400000     11\n",
       "24                                 Financial Services  0.508571     57\n",
       "25                                      Food Products  0.720588     46\n",
       "26                                      Gas Utilities  0.791667     16\n",
       "27                              Ground Transportation  0.376344     34\n",
       "28                   Health Care Equipment & Supplies  0.750000      5\n",
       "29                   Health Care Providers & Services  0.712610     42\n",
       "30                                  Health Care REITs  1.000000      7\n",
       "31                      Hotels, Restaurants & Leisure  0.802273     42\n",
       "32                                        IT Services  0.765385     36\n",
       "33  Independent Power & Renewable Electricity Prod...  0.626488     62\n",
       "34                                   Industrial REITs  0.825000     14\n",
       "35                                          Insurance  0.794534     71\n",
       "36                                   Leisure Products       NaN     16\n",
       "37                                          Machinery  0.860947     39\n",
       "38                              Marine Transportation       NaN      7\n",
       "39                                              Media  0.706731     34\n",
       "40                                    Metals & Mining  0.669971    413\n",
       "41                                    Multi-Utilities  0.336134     24\n",
       "42                                       Office REITs  0.178571     16\n",
       "43                        Oil, Gas & Consumable Fuels  0.593248    362\n",
       "44                            Paper & Forest Products  0.585000     45\n",
       "45                                 Passenger Airlines  0.657407     24\n",
       "46                             Personal Care Products       NaN      8\n",
       "47                                    Pharmaceuticals  0.785714     50\n",
       "48                              Professional Services  0.518519     30\n",
       "49               Real Estate Management & Development  0.753458    111\n",
       "50                                  Residential REITs  0.692308     37\n",
       "51                                       Retail REITs  0.594907     62\n",
       "52                                           Software  0.719313    108\n",
       "53                                  Specialized REITs       NaN      7\n",
       "54                                   Specialty Retail  0.826241     53\n",
       "55                   Textiles, Apparel & Luxury Goods  0.769231     16\n",
       "56                         Thrifts & Mortgage Finance  0.000000      7\n",
       "57                   Trading Companies & Distributors  0.485380     60\n",
       "58                      Transportation Infrastructure  0.125000     13\n",
       "59                Wireless Telecommunication Services  0.500000      9"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_auc_by_groups(\n",
    "    predictions_df=predictions_df,\n",
    "    df_canada=df_canada,\n",
    "    group_cols=[\"industry_raw\"],\n",
    "    out_csv=\"auc_par_industrie.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ns/nlrkcsjd7j70p9jvf2jl5t4c0000gn/T/ipykernel_1219/2025299659.py:53: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.716096</td>\n",
       "      <td>1428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year       AUC  Count\n",
       "0  2023  0.716096   1428\n",
       "1  2024       NaN   1416"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_auc_by_groups(\n",
    "    predictions_df=predictions_df,\n",
    "    df_canada=df_canada,\n",
    "    group_cols=[\"year\"],\n",
    "    out_csv=\"auc_par_annee.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création de Portefeuille + Annual Weighted Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weighted_portfolios(predictions_df, df_canada, proba_col, return_col, lower_threshold=0.4, upper_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Crée un portefeuille pondéré basé sur les prédictions et calcule les rendements pondérés par année,\n",
    "    en attribuant des poids positifs aux positions longues et négatifs aux positions courtes, avec une somme neutre.\n",
    "    \n",
    "    Args:\n",
    "    - predictions_df (pd.DataFrame): DataFrame contenant les prédictions et les identifiants 'cid'.\n",
    "    - df_canada (pd.DataFrame): DataFrame contenant les rendements futurs et les identifiants 'cid'.\n",
    "    - return_col (str): Nom de la colonne des rendements futurs dans df_canada.\n",
    "    - lower_threshold (float): Seuil inférieur pour les positions courtes.\n",
    "    - upper_threshold (float): Seuil supérieur pour les positions longues.\n",
    "    \n",
    "    Returns:\n",
    "    - result_df (pd.DataFrame): DataFrame contenant les rendements pondérés des portefeuilles par année.\n",
    "    \"\"\"\n",
    "    # Assure que les colonnes 'date' sont au bon format datetime dans les deux DataFrames\n",
    "    predictions_df['date'] = pd.to_datetime(predictions_df['date'])\n",
    "    df_canada['date'] = pd.to_datetime(df_canada['date'])\n",
    "        \n",
    "    \n",
    "    # Joindre les deux DataFrames sur 'cid' et 'date'\n",
    "    merged_df = predictions_df.merge(df_canada[['cid', 'date', return_col]], on=['cid', 'date'], how='left')\n",
    "    \n",
    "    # Extraire l'année à partir de la colonne 'date'\n",
    "    merged_df['year'] = merged_df['date'].dt.year\n",
    "    \n",
    "    # Retirer les lignes avec des valeurs manquantes\n",
    "    merged_df.dropna(inplace=True)\n",
    "    \n",
    "    # Initialiser une liste pour stocker les résultats\n",
    "    results = []\n",
    "\n",
    "    # Grouper par année\n",
    "    for year, group in merged_df.groupby('year'):\n",
    "        # Sélectionner les positions longues et courtes selon les seuils\n",
    "        selected_long = group[group[proba_col] > upper_threshold]  # Positions longues\n",
    "        selected_short = group[group[proba_col] < lower_threshold]  # Positions courtes\n",
    "        \n",
    "        n_long = len(selected_long)\n",
    "        n_short = len(selected_short)\n",
    "        \n",
    "        if n_long > 0 or n_short > 0:\n",
    "            # Attribuer des poids aux positions longues et courtes\n",
    "            if n_long > 0:\n",
    "                selected_long.loc[:, 'weight'] = 1 / n_long  # Poids positifs pour les positions longues\n",
    "            if n_short > 0:\n",
    "                selected_short.loc[:, 'weight'] = -1 / n_short  # Poids négatifs pour les positions courtes\n",
    "            \n",
    "            # Combiner les deux DataFrames\n",
    "            selected = pd.concat([selected_long, selected_short], ignore_index=True)\n",
    "            \n",
    "            # Calculer le rendement pondéré du portefeuille (somme des rendements pondérés)\n",
    "            weighted_return = (selected['weight'] * selected[return_col] * 100).sum()\n",
    "            \n",
    "            # Ajouter le résultat à la liste\n",
    "            results.append({'year': year, 'weighted_return': weighted_return})\n",
    "        else:\n",
    "            # Si aucune action ne respecte les seuils, le rendement est NaN\n",
    "            results.append({'year': year, 'weighted_return': float('nan')})\n",
    "\n",
    "    # Convertir les résultats en DataFrame\n",
    "    result_df = pd.DataFrame(results)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appel de fonction rendement annuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ns/nlrkcsjd7j70p9jvf2jl5t4c0000gn/T/ipykernel_1219/444120012.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/var/folders/ns/nlrkcsjd7j70p9jvf2jl5t4c0000gn/T/ipykernel_1219/444120012.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/var/folders/ns/nlrkcsjd7j70p9jvf2jl5t4c0000gn/T/ipykernel_1219/444120012.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/var/folders/ns/nlrkcsjd7j70p9jvf2jl5t4c0000gn/T/ipykernel_1219/444120012.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>weighted_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>-0.088229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>-1.622741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  weighted_return\n",
       "0  2023        -0.088229\n",
       "1  2024        -1.622741"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_net_income = create_weighted_portfolios(predictions_df, df_canada, 'net_prob_score', 'return_1q', lower_threshold=0.5, upper_threshold=0.5)\n",
    "\n",
    "#ret_net_income.to_csv(\"returns_net_income_0.5_0.5.csv\", index=False)\n",
    "\n",
    "ret_net_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ns/nlrkcsjd7j70p9jvf2jl5t4c0000gn/T/ipykernel_1219/444120012.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/var/folders/ns/nlrkcsjd7j70p9jvf2jl5t4c0000gn/T/ipykernel_1219/444120012.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/var/folders/ns/nlrkcsjd7j70p9jvf2jl5t4c0000gn/T/ipykernel_1219/444120012.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/var/folders/ns/nlrkcsjd7j70p9jvf2jl5t4c0000gn/T/ipykernel_1219/444120012.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>weighted_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>-0.725660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>-1.766371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  weighted_return\n",
       "0  2023        -0.725660\n",
       "1  2024        -1.766371"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns_net_income = create_weighted_portfolios(predictions_df, df_canada, 'net_prob_score', 'return_1q', lower_threshold=0.4, upper_threshold=0.6)\n",
    "\n",
    "#returns_net_income.to_csv(\"returns_net_income_0.4_0.6.csv\", index=False)\n",
    "\n",
    "returns_net_income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendement par décile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_decile_returns(\n",
    "    predictions_df: pd.DataFrame,\n",
    "    df_canada: pd.DataFrame,\n",
    "    proba_col: str,\n",
    "    return_col: str,\n",
    "    aggregator: str = \"mean\",        # \"mean\" ou \"median\"\n",
    "    decile_weights: list = None,     # liste de 10 poids ou None\n",
    "    csv_path: str = None             # chemin CSV ou None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calcule le rendement par décile (du plus faible au plus élevé) pour chaque année.\n",
    "    - Découpage en déciles via pd.qcut (q=10).\n",
    "    - Agrégation des rendements par décile via un 'aggregator' (mean ou median).\n",
    "    - Optionnellement, applique un vecteur de poids (decile_weights) pour calculer\n",
    "      un rendement global (somme pondérée des déciles).\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    predictions_df : pd.DataFrame\n",
    "        Contient les prédictions (probabilités) et les colonnes 'cid', 'date'.\n",
    "        Doit inclure la colonne `proba_col`.\n",
    "    df_canada : pd.DataFrame\n",
    "        Contient la colonne `return_col` ainsi que 'cid', 'date'.\n",
    "    proba_col : str\n",
    "        Nom de la colonne dans predictions_df représentant la probabilité ou le score.\n",
    "    return_col : str\n",
    "        Nom de la colonne dans df_canada qui contient le rendement (ou variation).\n",
    "    aggregator : {\"mean\", \"median\"}, défaut = \"mean\"\n",
    "        Choix entre la moyenne ou la médiane comme statistique de rendement pour le décile.\n",
    "    decile_weights : list of float (length 10) ou None\n",
    "        Si fourni, liste de 10 poids (un pour chaque décile de 0 à 9).\n",
    "        Permet de calculer une colonne \"portfolio_return\" = somme(décile_i * poids_i).\n",
    "    csv_path : str ou None\n",
    "        Si un chemin est fourni, le DataFrame final sera sauvegardé en CSV.\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    df_final : pd.DataFrame\n",
    "        DataFrame pivoté avec:\n",
    "        - 1 ligne par année\n",
    "        - 10 colonnes (D1 à D10) indiquant le rendement agrégé de chaque décile\n",
    "        - Si decile_weights est fourni, une colonne supplémentaire 'portfolio_return'\n",
    "          représentant la somme pondérée des rendements déciles.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1) Merge sur cid + date\n",
    "    merged_df = predictions_df.merge(\n",
    "        df_canada[['cid', 'date', return_col]],\n",
    "        on=['cid', 'date'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # --- 2) Extraire l'année\n",
    "    merged_df['year'] = merged_df['date'].dt.year\n",
    "    \n",
    "    # --- 3) Nettoyage éventuel\n",
    "    merged_df.dropna(subset=[proba_col, return_col, 'year'], inplace=True)\n",
    "    \n",
    "    # --- 4) Choix de la fonction d'agrégation\n",
    "    if aggregator == \"mean\":\n",
    "        aggregator_func = np.mean\n",
    "    elif aggregator == \"median\":\n",
    "        aggregator_func = np.median\n",
    "    else:\n",
    "        raise ValueError(\"aggregator must be 'mean' or 'median'.\")\n",
    "    \n",
    "    # --- 5) Préparer une liste pour stocker (year, decile_bin, agg_return)\n",
    "    decile_results = []\n",
    "    \n",
    "    for year, group in merged_df.groupby('year'):\n",
    "        # On doit découper en 10 déciles\n",
    "        group = group.copy()\n",
    "        \n",
    "        # Astuce: labels=False renvoie des labels 0..9\n",
    "        group['decile_bin'] = pd.qcut(group[proba_col], q=10, labels=False)\n",
    "        \n",
    "        # Calculer l'agrégat (mean ou median) du rendement dans chaque décile\n",
    "        for decile_id, subdf in group.groupby('decile_bin'):\n",
    "            if pd.isnull(decile_id):\n",
    "                continue\n",
    "            \n",
    "            decile_id = int(decile_id)\n",
    "            # Rendement agrégé (en % si on veut)\n",
    "            decile_value = aggregator_func(subdf[return_col]) * 100.0\n",
    "            \n",
    "            decile_results.append({\n",
    "                'year': year,\n",
    "                'decile': decile_id,\n",
    "                'decile_return': decile_value\n",
    "            })\n",
    "    \n",
    "    # --- 6) On transforme la liste en DataFrame + pivot\n",
    "    decile_df = pd.DataFrame(decile_results)\n",
    "    df_pivot = decile_df.pivot(index='year', columns='decile', values='decile_return')\n",
    "    \n",
    "    # Renommer les colonnes pour D1..D10\n",
    "    # Attention : decile_id = 0 correspond au 1er décile => D1\n",
    "    df_pivot.columns = [f\"D{i+1}\" for i in df_pivot.columns]\n",
    "    \n",
    "    # --- 7) Si on a des poids, on calcule la somme pondérée\n",
    "    if decile_weights is not None:\n",
    "        if len(decile_weights) != 10:\n",
    "            raise ValueError(\"decile_weights must be a list of length 10.\")\n",
    "        \n",
    "        # Multiplier chaque colonne D1..D10 par le poids correspondant, puis sommer\n",
    "        # Astuce : df_pivot.iloc[:, 0:10] prend les 10 premières colonnes => D1..D10\n",
    "        # On peut ensuite .mul(decile_weights, axis='columns') pour multiplier par le vecteur\n",
    "        # et .sum(axis=1) pour obtenir la somme par année\n",
    "        # (on suppose que l'ordre des colonnes est D1..D10).\n",
    "        \n",
    "        weighted_sum = df_pivot.iloc[:, 0:10].mul(decile_weights, axis='columns').sum(axis=1)\n",
    "        \n",
    "        # On l'ajoute comme nouvelle colonne\n",
    "        df_pivot['portfolio_return'] = weighted_sum\n",
    "    \n",
    "    # --- 8) On remet l'index 'year' comme colonne\n",
    "    df_final = df_pivot.reset_index()\n",
    "    \n",
    "    # --- 9) Sauvegarde CSV si demandé\n",
    "    if csv_path is not None:\n",
    "        df_final.to_csv(csv_path, index=False)\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>D10</th>\n",
       "      <th>portfolio_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.522507</td>\n",
       "      <td>0.683460</td>\n",
       "      <td>1.737135</td>\n",
       "      <td>2.468283</td>\n",
       "      <td>5.336049</td>\n",
       "      <td>3.263129</td>\n",
       "      <td>2.187534</td>\n",
       "      <td>4.934321</td>\n",
       "      <td>0.774357</td>\n",
       "      <td>0.919068</td>\n",
       "      <td>2.609937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>4.591108</td>\n",
       "      <td>6.823602</td>\n",
       "      <td>4.750542</td>\n",
       "      <td>3.774460</td>\n",
       "      <td>4.064164</td>\n",
       "      <td>4.586458</td>\n",
       "      <td>2.925520</td>\n",
       "      <td>4.973091</td>\n",
       "      <td>3.571028</td>\n",
       "      <td>0.919241</td>\n",
       "      <td>2.367461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year        D1        D2        D3        D4        D5        D6        D7  \\\n",
       "0  2023  0.522507  0.683460  1.737135  2.468283  5.336049  3.263129  2.187534   \n",
       "1  2024  4.591108  6.823602  4.750542  3.774460  4.064164  4.586458  2.925520   \n",
       "\n",
       "         D8        D9       D10  portfolio_return  \n",
       "0  4.934321  0.774357  0.919068          2.609937  \n",
       "1  4.973091  3.571028  0.919241          2.367461  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_deciles = compute_decile_returns(\n",
    "    predictions_df=predictions_df,\n",
    "    df_canada=df_canada,\n",
    "    proba_col='net_prob_score',\n",
    "    return_col='return_1q',\n",
    "    aggregator='mean',\n",
    "    decile_weights=[-0.1, -0.1, 0, 0, 0.1, 0.2, 0.1, 0.2, 0.2, 0.2],\n",
    "    csv_path='my_decile_returns.csv'\n",
    ")\n",
    "\n",
    "df_deciles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preuve CSV Rendement par Décile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Merge sur cid + date\n",
    "merged_df = predictions_df.merge(\n",
    "    df_canada[['cid', 'date', 'return_1q']],\n",
    "    on=['cid', 'date'],\n",
    "    how='left'\n",
    "    )\n",
    "\n",
    "# --- 2) Extraire l'année\n",
    "merged_df['year'] = merged_df['date'].dt.year\n",
    "\n",
    "# Filtrer pour l'année 2023\n",
    "year = 2023\n",
    "year_df = merged_df[merged_df['year'] == year].copy()\n",
    "\n",
    "# Attribuer les déciles pour cette année\n",
    "year_df['decile_bin'] = pd.qcut(year_df['net_prob_score'], q=10, labels=False)\n",
    "\n",
    "# Liste pour stocker chaque subdf avec une colonne indiquant le décile\n",
    "list_of_subdfs = []\n",
    "\n",
    "# Itérer sur chaque décile et collecter les sous-ensembles\n",
    "for decile_id, subdf in year_df.groupby('decile_bin'):\n",
    "    # Ajouter une colonne indiquant le décile courant\n",
    "    subdf = subdf.copy()\n",
    "    subdf['decile_id'] = decile_id\n",
    "    list_of_subdfs.append(subdf)\n",
    "\n",
    "# Concaténer tous les subdf en un seul DataFrame\n",
    "all_subdfs_df = pd.concat(list_of_subdfs, ignore_index=True)\n",
    "\n",
    "# Exporter vers CSV\n",
    "all_subdfs_df.to_csv(\"subdfs_2023.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BackUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weighted_portfolios_backup(\n",
    "    predictions_df, \n",
    "    df_canada, \n",
    "    proba_col, \n",
    "    return_col, \n",
    "    lower_threshold=0.4, \n",
    "    upper_threshold=0.6,\n",
    "    group_mode=None,  # \"decile\" ou \"binning\" ou None\n",
    "    return_decile_or_bins=False,\n",
    "    debug_bins=False  # <-- Paramètre pour afficher les stats des bins \n",
    "):\n",
    "    \"\"\"\n",
    "    Exemple d'une fonction qui calcule un portefeuille\n",
    "    + rendements par déciles ou binning.\n",
    "    Et qui, si debug_bins=True, affiche la répartition des probabilités dans chaque bin.\n",
    "    \"\"\"\n",
    "    \n",
    "    merged_df = predictions_df.merge(df_canada[['cid', 'date', return_col]], \n",
    "                                     on=['cid', 'date'], how='left')\n",
    "    \n",
    "    merged_df['year'] = merged_df['date'].dt.year\n",
    "    merged_df.dropna(subset=[proba_col, return_col, 'year'], inplace=True)\n",
    "    \n",
    "    # --- 1) Calcul du portefeuille \"classique\" (logique seuils) ---\n",
    "    results = []\n",
    "    for year, group in merged_df.groupby('year'):\n",
    "        selected_long = group[group[proba_col] > upper_threshold].copy()\n",
    "        selected_short = group[group[proba_col] < lower_threshold].copy()\n",
    "        \n",
    "        n_long = len(selected_long)\n",
    "        n_short = len(selected_short)\n",
    "        \n",
    "        if n_long > 0 or n_short > 0:\n",
    "            if n_long > 0:\n",
    "                selected_long['weight'] = 1.0 / n_long\n",
    "            if n_short > 0:\n",
    "                selected_short['weight'] = -1.0 / n_short\n",
    "            \n",
    "            selected = pd.concat([selected_long, selected_short], ignore_index=True)\n",
    "            weighted_return = (selected['weight'] * selected[return_col] * 100).sum()\n",
    "            \n",
    "            results.append({'year': year, 'weighted_return': weighted_return})\n",
    "        else:\n",
    "            results.append({'year': year, 'weighted_return': float('nan')})\n",
    "    \n",
    "    result_df = pd.DataFrame(results)\n",
    "    \n",
    "    # --- 2) Calcul par déciles ou binning (optionnel) ---\n",
    "    decile_or_bin_returns_df = None\n",
    "    if return_decile_or_bins and group_mode is not None:\n",
    "        decile_or_bin_list = []\n",
    "        \n",
    "        for year, group in merged_df.groupby('year'):\n",
    "            group = group.copy().reset_index(drop=True)\n",
    "            \n",
    "            # Construire les bins/déciles\n",
    "            if group_mode == \"decile\":\n",
    "                group['decile_bin'] = pd.qcut(group[proba_col], q=10, labels=False)\n",
    "            elif group_mode == \"binning\":\n",
    "                bins = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "                group['decile_bin'] = pd.cut(group[proba_col], bins=bins, labels=False, include_lowest=True)\n",
    "            else:\n",
    "                raise ValueError(\"group_mode must be either 'decile' or 'binning'\")\n",
    "            \n",
    "            # -- Si on veut déboguer / visualiser les bins --\n",
    "            #if debug_bins:\n",
    "                #print(f\"\\n=== Année {year} : Répartition des probabilités par bin ===\")\n",
    "                # Regrouper par decile_bin et afficher min, max, count\n",
    "                #bin_stats = group.groupby('decile_bin')[proba_col].agg(['min','max','count'])\n",
    "                #print(bin_stats)\n",
    "                #bin_all_probas = group.groupby('decile_bin')[proba_col].apply(list)\n",
    "                #print(bin_all_probas)\n",
    "            \n",
    "            # Calculer le rendement par bin/décile\n",
    "            for bin_id, subdf in group.groupby('decile_bin'):\n",
    "                if pd.isnull(bin_id) or len(subdf) == 0:\n",
    "                    continue\n",
    "                \n",
    "                bin_id = int(bin_id)\n",
    "                \n",
    "                # Déterminer si ce bin/décile est long ou short via la moyenne\n",
    "                proba_mean = subdf[proba_col].mean()\n",
    "                if proba_mean < 0.5:\n",
    "                    w = -1.0 / len(subdf)\n",
    "                else:\n",
    "                    w = 1.0 / len(subdf)\n",
    "                \n",
    "                bin_return = (subdf[return_col] * w * 100).sum()\n",
    "                \n",
    "                decile_or_bin_list.append({\n",
    "                    'year': year,\n",
    "                    'decile_bin': bin_id,\n",
    "                    'weighted_return': bin_return\n",
    "                })\n",
    "        \n",
    "        decile_or_bin_df = pd.DataFrame(decile_or_bin_list)\n",
    "        decile_or_bin_returns_df = decile_or_bin_df.pivot(index='year', \n",
    "                                                          columns='decile_bin', \n",
    "                                                          values='weighted_return')\n",
    "        \n",
    "        # Renommer les colonnes\n",
    "        if group_mode == \"decile\":\n",
    "            decile_or_bin_returns_df.columns = [f'D{c+1}' for c in decile_or_bin_returns_df.columns]\n",
    "        else:\n",
    "            decile_or_bin_returns_df.columns = [f'Bin{c}' for c in decile_or_bin_returns_df.columns]\n",
    "        \n",
    "        decile_or_bin_returns_df.reset_index(inplace=True)\n",
    "    \n",
    "        return result_df, decile_or_bin_returns_df\n",
    "    else:\n",
    "        return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   year  weighted_return\n",
       " 0  2023         0.492533\n",
       " 1  2024        -3.000721,\n",
       "    year        D1        D2        D3        D4        D5        D6        D7  \\\n",
       " 0  2023 -0.522507 -0.683460 -1.737135 -2.468283 -5.336049 -3.263129 -2.187534   \n",
       " 1  2024 -4.591108 -6.823602 -4.750542 -3.774460 -4.064164 -4.586458 -2.925520   \n",
       " \n",
       "          D8        D9       D10  \n",
       " 0  4.934321  0.774357  0.919068  \n",
       " 1  4.973091  3.571028  0.919241  )"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_binning = create_weighted_portfolios_backup(predictions_df, df_canada, 'net_prob_score', 'return_1q', lower_threshold=-0.5, upper_threshold=0.6, group_mode=\"decile\", return_decile_or_bins=True, debug_bins=True)\n",
    "\n",
    "#test_binning.to_csv(\"returns_net_income_0.5_0.5.csv\", index=False)\n",
    "\n",
    "test_binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HECFinance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
